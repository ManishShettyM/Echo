{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa import display\n",
    "import pandas as pd\n",
    "import glob \n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Dataset/Audio_Speech_Actors_01-24\"\n",
    "feeling_list = []\n",
    "for root,dirs,files in os.walk(path):\n",
    "    if(len(files)>0):\n",
    "        for file in files:\n",
    "            if(int(file.split(\".\")[0].split(\"-\")[2]) == 1):\n",
    "                feeling_list.append('neutral')\n",
    "            elif(int(file.split(\".\")[0].split(\"-\")[2]) == 2):\n",
    "                feeling_list.append('calm')\n",
    "            elif(int(file.split(\".\")[0].split(\"-\")[2]) == 3):\n",
    "                feeling_list.append('happy')\n",
    "            elif(int(file.split(\".\")[0].split(\"-\")[2]) == 4):\n",
    "                feeling_list.append('sad')\n",
    "            elif(int(file.split(\".\")[0].split(\"-\")[2]) == 5):\n",
    "                feeling_list.append('angry')\n",
    "            elif(int(file.split(\".\")[0].split(\"-\")[2]) == 6):\n",
    "                feeling_list.append('fearful')\n",
    "            elif(int(file.split(\".\")[0].split(\"-\")[2]) == 7):\n",
    "                feeling_list.append('disgust')\n",
    "            elif(int(file.split(\".\")[0].split(\"-\")[2]) == 8):\n",
    "                feeling_list.append('surprised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "0        sad\n",
       "1        sad\n",
       "2      happy\n",
       "3  surprised\n",
       "4      happy\n",
       "5       calm\n",
       "6       calm\n",
       "7      angry\n",
       "8    neutral\n",
       "9    fearful"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(feeling_list))\n",
    "labels = pd.DataFrame(feeling_list , columns=['label'])\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark = 0\n",
    "for subdir,dirs,files in os.walk(path):\n",
    "    for file in files:\n",
    "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "        sample_rate = np.array(sample_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                            sr=sample_rate, \n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark=bookmark+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-71.80572946267719, -71.80572946267719, -71.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-57.983965439717444, -58.12975730353416, -57....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-51.123276362992605, -51.8187817185881, -51.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-59.1965370795561, -59.1965370795561, -59.196...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-43.633354817987765, -45.07731733923093, -47....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature\n",
       "0  [-71.80572946267719, -71.80572946267719, -71.8...\n",
       "1  [-57.983965439717444, -58.12975730353416, -57....\n",
       "2  [-51.123276362992605, -51.8187817185881, -51.9...\n",
       "3  [-59.1965370795561, -59.1965370795561, -59.196...\n",
       "4  [-43.633354817987765, -45.07731733923093, -47...."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(df['feature'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.concat([df3,labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>...</td>\n",
       "      <td>-48.080450</td>\n",
       "      <td>-48.708313</td>\n",
       "      <td>-47.572926</td>\n",
       "      <td>-48.468573</td>\n",
       "      <td>-48.897654</td>\n",
       "      <td>-50.247077</td>\n",
       "      <td>-49.038084</td>\n",
       "      <td>-48.715561</td>\n",
       "      <td>-51.587351</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-57.983965</td>\n",
       "      <td>-58.129757</td>\n",
       "      <td>-57.216111</td>\n",
       "      <td>-57.138616</td>\n",
       "      <td>-58.032113</td>\n",
       "      <td>-57.584437</td>\n",
       "      <td>-57.990391</td>\n",
       "      <td>-57.497483</td>\n",
       "      <td>-57.414894</td>\n",
       "      <td>-58.088535</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.392082</td>\n",
       "      <td>-40.890291</td>\n",
       "      <td>-42.336407</td>\n",
       "      <td>-43.509533</td>\n",
       "      <td>-42.348652</td>\n",
       "      <td>-42.181487</td>\n",
       "      <td>-45.076798</td>\n",
       "      <td>-45.154829</td>\n",
       "      <td>-39.698874</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-51.123276</td>\n",
       "      <td>-51.818782</td>\n",
       "      <td>-51.941592</td>\n",
       "      <td>-52.816664</td>\n",
       "      <td>-54.652136</td>\n",
       "      <td>-56.915060</td>\n",
       "      <td>-57.323525</td>\n",
       "      <td>-56.916718</td>\n",
       "      <td>-54.595758</td>\n",
       "      <td>-54.187025</td>\n",
       "      <td>...</td>\n",
       "      <td>-50.799301</td>\n",
       "      <td>-50.775673</td>\n",
       "      <td>-51.096929</td>\n",
       "      <td>-51.170862</td>\n",
       "      <td>-50.121700</td>\n",
       "      <td>-49.047896</td>\n",
       "      <td>-49.425883</td>\n",
       "      <td>-50.382450</td>\n",
       "      <td>-53.368919</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>...</td>\n",
       "      <td>-48.255887</td>\n",
       "      <td>-48.819326</td>\n",
       "      <td>-49.072596</td>\n",
       "      <td>-49.837795</td>\n",
       "      <td>-50.631560</td>\n",
       "      <td>-49.627614</td>\n",
       "      <td>-49.795650</td>\n",
       "      <td>-49.463476</td>\n",
       "      <td>-49.567603</td>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.633355</td>\n",
       "      <td>-45.077317</td>\n",
       "      <td>-47.110537</td>\n",
       "      <td>-47.066368</td>\n",
       "      <td>-46.836029</td>\n",
       "      <td>-46.536989</td>\n",
       "      <td>-47.593565</td>\n",
       "      <td>-47.908471</td>\n",
       "      <td>-48.006105</td>\n",
       "      <td>-47.703133</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.769527</td>\n",
       "      <td>-16.043667</td>\n",
       "      <td>-14.923637</td>\n",
       "      <td>-14.839569</td>\n",
       "      <td>-15.081686</td>\n",
       "      <td>-17.688219</td>\n",
       "      <td>-18.029030</td>\n",
       "      <td>-17.959716</td>\n",
       "      <td>-18.121766</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5  \\\n",
       "0 -71.805729 -71.805729 -71.805729 -71.805729 -71.805729 -71.805729   \n",
       "1 -57.983965 -58.129757 -57.216111 -57.138616 -58.032113 -57.584437   \n",
       "2 -51.123276 -51.818782 -51.941592 -52.816664 -54.652136 -56.915060   \n",
       "3 -59.196537 -59.196537 -59.196537 -59.196537 -59.196537 -59.196537   \n",
       "4 -43.633355 -45.077317 -47.110537 -47.066368 -46.836029 -46.536989   \n",
       "\n",
       "           6          7          8          9    ...            207  \\\n",
       "0 -71.805729 -71.805729 -71.805729 -71.805729    ...     -48.080450   \n",
       "1 -57.990391 -57.497483 -57.414894 -58.088535    ...     -39.392082   \n",
       "2 -57.323525 -56.916718 -54.595758 -54.187025    ...     -50.799301   \n",
       "3 -59.196537 -59.196537 -59.196537 -59.196537    ...     -48.255887   \n",
       "4 -47.593565 -47.908471 -48.006105 -47.703133    ...     -17.769527   \n",
       "\n",
       "         208        209        210        211        212        213  \\\n",
       "0 -48.708313 -47.572926 -48.468573 -48.897654 -50.247077 -49.038084   \n",
       "1 -40.890291 -42.336407 -43.509533 -42.348652 -42.181487 -45.076798   \n",
       "2 -50.775673 -51.096929 -51.170862 -50.121700 -49.047896 -49.425883   \n",
       "3 -48.819326 -49.072596 -49.837795 -50.631560 -49.627614 -49.795650   \n",
       "4 -16.043667 -14.923637 -14.839569 -15.081686 -17.688219 -18.029030   \n",
       "\n",
       "         214        215      label  \n",
       "0 -48.715561 -51.587351        sad  \n",
       "1 -45.154829 -39.698874        sad  \n",
       "2 -50.382450 -53.368919      happy  \n",
       "3 -49.463476 -49.567603  surprised  \n",
       "4 -17.959716 -18.121766      happy  \n",
       "\n",
       "[5 rows x 217 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.to_csv(\"mfccFeatures_emotion_latest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>-54.511486</td>\n",
       "      <td>-53.330406</td>\n",
       "      <td>-52.614726</td>\n",
       "      <td>-55.296237</td>\n",
       "      <td>-58.634653</td>\n",
       "      <td>-58.468420</td>\n",
       "      <td>-56.901425</td>\n",
       "      <td>-57.517333</td>\n",
       "      <td>-55.310247</td>\n",
       "      <td>-51.296041</td>\n",
       "      <td>...</td>\n",
       "      <td>-44.802806</td>\n",
       "      <td>-44.482723</td>\n",
       "      <td>-43.834654</td>\n",
       "      <td>-45.097868</td>\n",
       "      <td>-46.659864</td>\n",
       "      <td>-46.247735</td>\n",
       "      <td>-47.877711</td>\n",
       "      <td>-47.970408</td>\n",
       "      <td>-44.807865</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>-56.357233</td>\n",
       "      <td>-55.912082</td>\n",
       "      <td>-53.390019</td>\n",
       "      <td>-54.204491</td>\n",
       "      <td>-51.211485</td>\n",
       "      <td>-51.550976</td>\n",
       "      <td>-50.826821</td>\n",
       "      <td>-50.699000</td>\n",
       "      <td>-51.794851</td>\n",
       "      <td>-52.526187</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.346369</td>\n",
       "      <td>-52.856998</td>\n",
       "      <td>-52.708602</td>\n",
       "      <td>-52.077265</td>\n",
       "      <td>-50.171066</td>\n",
       "      <td>-49.525395</td>\n",
       "      <td>-50.240926</td>\n",
       "      <td>-52.254960</td>\n",
       "      <td>-54.420155</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>-41.033862</td>\n",
       "      <td>-43.473726</td>\n",
       "      <td>-44.572001</td>\n",
       "      <td>-45.404978</td>\n",
       "      <td>-45.459836</td>\n",
       "      <td>-45.979375</td>\n",
       "      <td>-46.638765</td>\n",
       "      <td>-45.706508</td>\n",
       "      <td>-45.103673</td>\n",
       "      <td>-45.049784</td>\n",
       "      <td>...</td>\n",
       "      <td>-36.153344</td>\n",
       "      <td>-35.001036</td>\n",
       "      <td>-35.734099</td>\n",
       "      <td>-34.929305</td>\n",
       "      <td>-33.435040</td>\n",
       "      <td>-32.902015</td>\n",
       "      <td>-32.227553</td>\n",
       "      <td>-32.627929</td>\n",
       "      <td>-30.547536</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-58.545328</td>\n",
       "      <td>-59.202478</td>\n",
       "      <td>-60.137033</td>\n",
       "      <td>-62.037020</td>\n",
       "      <td>-62.147955</td>\n",
       "      <td>-61.235847</td>\n",
       "      <td>-61.505283</td>\n",
       "      <td>-61.574950</td>\n",
       "      <td>-61.505381</td>\n",
       "      <td>-60.495135</td>\n",
       "      <td>...</td>\n",
       "      <td>-62.534837</td>\n",
       "      <td>-63.489974</td>\n",
       "      <td>-63.011312</td>\n",
       "      <td>-60.754591</td>\n",
       "      <td>-60.942248</td>\n",
       "      <td>-61.236054</td>\n",
       "      <td>-61.601959</td>\n",
       "      <td>-60.400122</td>\n",
       "      <td>-60.975515</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>-49.499418</td>\n",
       "      <td>-48.170479</td>\n",
       "      <td>-49.352273</td>\n",
       "      <td>-52.664401</td>\n",
       "      <td>-50.219882</td>\n",
       "      <td>-49.810335</td>\n",
       "      <td>-49.938510</td>\n",
       "      <td>-50.251170</td>\n",
       "      <td>-44.789156</td>\n",
       "      <td>-40.580678</td>\n",
       "      <td>...</td>\n",
       "      <td>-47.737491</td>\n",
       "      <td>-49.919034</td>\n",
       "      <td>-49.157083</td>\n",
       "      <td>-48.099718</td>\n",
       "      <td>-48.619908</td>\n",
       "      <td>-49.115151</td>\n",
       "      <td>-47.950252</td>\n",
       "      <td>-48.969603</td>\n",
       "      <td>-49.690079</td>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>-53.678315</td>\n",
       "      <td>-54.377914</td>\n",
       "      <td>-55.140844</td>\n",
       "      <td>-56.300240</td>\n",
       "      <td>-55.326932</td>\n",
       "      <td>-55.119810</td>\n",
       "      <td>-56.282534</td>\n",
       "      <td>-54.970338</td>\n",
       "      <td>-53.334915</td>\n",
       "      <td>-52.519249</td>\n",
       "      <td>...</td>\n",
       "      <td>-50.354847</td>\n",
       "      <td>-51.740110</td>\n",
       "      <td>-53.151655</td>\n",
       "      <td>-53.596402</td>\n",
       "      <td>-53.313462</td>\n",
       "      <td>-53.512526</td>\n",
       "      <td>-55.573837</td>\n",
       "      <td>-56.696713</td>\n",
       "      <td>-56.354921</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-67.252646</td>\n",
       "      <td>...</td>\n",
       "      <td>-64.678076</td>\n",
       "      <td>-64.360786</td>\n",
       "      <td>-64.642809</td>\n",
       "      <td>-62.274782</td>\n",
       "      <td>-62.417866</td>\n",
       "      <td>-63.560128</td>\n",
       "      <td>-63.105106</td>\n",
       "      <td>-63.359077</td>\n",
       "      <td>-63.284678</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>...</td>\n",
       "      <td>-46.490646</td>\n",
       "      <td>-46.694752</td>\n",
       "      <td>-48.137768</td>\n",
       "      <td>-47.240571</td>\n",
       "      <td>-47.443421</td>\n",
       "      <td>-45.964384</td>\n",
       "      <td>-47.454131</td>\n",
       "      <td>-44.490975</td>\n",
       "      <td>-44.662811</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>-70.152745</td>\n",
       "      <td>-69.913691</td>\n",
       "      <td>-67.810702</td>\n",
       "      <td>-68.664977</td>\n",
       "      <td>-71.470452</td>\n",
       "      <td>-71.470452</td>\n",
       "      <td>-71.470452</td>\n",
       "      <td>-71.470452</td>\n",
       "      <td>-71.470452</td>\n",
       "      <td>-71.470452</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.779465</td>\n",
       "      <td>-43.768887</td>\n",
       "      <td>-44.085496</td>\n",
       "      <td>-43.975650</td>\n",
       "      <td>-44.347019</td>\n",
       "      <td>-44.098364</td>\n",
       "      <td>-46.298862</td>\n",
       "      <td>-47.547014</td>\n",
       "      <td>-46.484299</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>-55.243801</td>\n",
       "      <td>-56.378484</td>\n",
       "      <td>-57.449357</td>\n",
       "      <td>-57.225963</td>\n",
       "      <td>-57.018199</td>\n",
       "      <td>-55.223716</td>\n",
       "      <td>-53.239328</td>\n",
       "      <td>-52.637030</td>\n",
       "      <td>-52.636234</td>\n",
       "      <td>-53.478285</td>\n",
       "      <td>...</td>\n",
       "      <td>-35.352025</td>\n",
       "      <td>-34.598943</td>\n",
       "      <td>-34.833487</td>\n",
       "      <td>-32.159613</td>\n",
       "      <td>-30.491879</td>\n",
       "      <td>-31.179180</td>\n",
       "      <td>-29.126147</td>\n",
       "      <td>-20.525918</td>\n",
       "      <td>-15.224331</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1          2          3          4          5  \\\n",
       "652  -54.511486 -53.330406 -52.614726 -55.296237 -58.634653 -58.468420   \n",
       "524  -56.357233 -55.912082 -53.390019 -54.204491 -51.211485 -51.550976   \n",
       "470  -41.033862 -43.473726 -44.572001 -45.404978 -45.459836 -45.979375   \n",
       "11   -58.545328 -59.202478 -60.137033 -62.037020 -62.147955 -61.235847   \n",
       "277  -49.499418 -48.170479 -49.352273 -52.664401 -50.219882 -49.810335   \n",
       "621  -53.678315 -54.377914 -55.140844 -56.300240 -55.326932 -55.119810   \n",
       "773  -64.968548 -64.968548 -64.968548 -64.968548 -64.968548 -64.968548   \n",
       "1357 -59.376024 -59.376024 -59.376024 -59.376024 -59.376024 -59.376024   \n",
       "170  -70.152745 -69.913691 -67.810702 -68.664977 -71.470452 -71.470452   \n",
       "1174 -55.243801 -56.378484 -57.449357 -57.225963 -57.018199 -55.223716   \n",
       "\n",
       "              6          7          8          9    ...            207  \\\n",
       "652  -56.901425 -57.517333 -55.310247 -51.296041    ...     -44.802806   \n",
       "524  -50.826821 -50.699000 -51.794851 -52.526187    ...     -52.346369   \n",
       "470  -46.638765 -45.706508 -45.103673 -45.049784    ...     -36.153344   \n",
       "11   -61.505283 -61.574950 -61.505381 -60.495135    ...     -62.534837   \n",
       "277  -49.938510 -50.251170 -44.789156 -40.580678    ...     -47.737491   \n",
       "621  -56.282534 -54.970338 -53.334915 -52.519249    ...     -50.354847   \n",
       "773  -64.968548 -64.968548 -64.968548 -67.252646    ...     -64.678076   \n",
       "1357 -59.376024 -59.376024 -59.376024 -59.376024    ...     -46.490646   \n",
       "170  -71.470452 -71.470452 -71.470452 -71.470452    ...     -43.779465   \n",
       "1174 -53.239328 -52.637030 -52.636234 -53.478285    ...     -35.352025   \n",
       "\n",
       "            208        209        210        211        212        213  \\\n",
       "652  -44.482723 -43.834654 -45.097868 -46.659864 -46.247735 -47.877711   \n",
       "524  -52.856998 -52.708602 -52.077265 -50.171066 -49.525395 -50.240926   \n",
       "470  -35.001036 -35.734099 -34.929305 -33.435040 -32.902015 -32.227553   \n",
       "11   -63.489974 -63.011312 -60.754591 -60.942248 -61.236054 -61.601959   \n",
       "277  -49.919034 -49.157083 -48.099718 -48.619908 -49.115151 -47.950252   \n",
       "621  -51.740110 -53.151655 -53.596402 -53.313462 -53.512526 -55.573837   \n",
       "773  -64.360786 -64.642809 -62.274782 -62.417866 -63.560128 -63.105106   \n",
       "1357 -46.694752 -48.137768 -47.240571 -47.443421 -45.964384 -47.454131   \n",
       "170  -43.768887 -44.085496 -43.975650 -44.347019 -44.098364 -46.298862   \n",
       "1174 -34.598943 -34.833487 -32.159613 -30.491879 -31.179180 -29.126147   \n",
       "\n",
       "            214        215      label  \n",
       "652  -47.970408 -44.807865        sad  \n",
       "524  -52.254960 -54.420155      happy  \n",
       "470  -32.627929 -30.547536    fearful  \n",
       "11   -60.400122 -60.975515    fearful  \n",
       "277  -48.969603 -49.690079  surprised  \n",
       "621  -56.696713 -56.354921      angry  \n",
       "773  -63.359077 -63.284678      happy  \n",
       "1357 -44.490975 -44.662811    disgust  \n",
       "170  -47.547014 -46.484299       calm  \n",
       "1174 -20.525918 -15.224331    disgust  \n",
       "\n",
       "[10 rows x 217 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "newdf = shuffle(newdf)\n",
    "newdf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf=newdf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf1 = np.random.rand(len(rnewdf)) < 0.8\n",
    "train = rnewdf[newdf1]\n",
    "test = rnewdf[~newdf1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-53.756179</td>\n",
       "      <td>-49.120491</td>\n",
       "      <td>-47.523206</td>\n",
       "      <td>-49.048604</td>\n",
       "      <td>-51.008211</td>\n",
       "      <td>-51.015084</td>\n",
       "      <td>-50.169745</td>\n",
       "      <td>-50.886473</td>\n",
       "      <td>-50.275355</td>\n",
       "      <td>-51.907351</td>\n",
       "      <td>...</td>\n",
       "      <td>-47.518230</td>\n",
       "      <td>-46.052274</td>\n",
       "      <td>-45.574740</td>\n",
       "      <td>-46.517232</td>\n",
       "      <td>-48.663554</td>\n",
       "      <td>-49.160694</td>\n",
       "      <td>-51.180327</td>\n",
       "      <td>-47.262994</td>\n",
       "      <td>-45.473499</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-48.351829</td>\n",
       "      <td>-47.869963</td>\n",
       "      <td>-46.828234</td>\n",
       "      <td>-49.911606</td>\n",
       "      <td>-50.994150</td>\n",
       "      <td>-47.716347</td>\n",
       "      <td>-47.951298</td>\n",
       "      <td>-50.378521</td>\n",
       "      <td>-49.107214</td>\n",
       "      <td>-50.084609</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.069463</td>\n",
       "      <td>-27.181224</td>\n",
       "      <td>-28.187199</td>\n",
       "      <td>-28.794791</td>\n",
       "      <td>-28.706694</td>\n",
       "      <td>-30.295817</td>\n",
       "      <td>-31.448918</td>\n",
       "      <td>-29.981349</td>\n",
       "      <td>-29.475319</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-61.363933</td>\n",
       "      <td>-60.424969</td>\n",
       "      <td>-60.542148</td>\n",
       "      <td>-63.737012</td>\n",
       "      <td>-64.673218</td>\n",
       "      <td>-61.401023</td>\n",
       "      <td>-62.801707</td>\n",
       "      <td>-64.522762</td>\n",
       "      <td>-64.676879</td>\n",
       "      <td>-63.780731</td>\n",
       "      <td>...</td>\n",
       "      <td>-58.870220</td>\n",
       "      <td>-61.683645</td>\n",
       "      <td>-59.392167</td>\n",
       "      <td>-58.266269</td>\n",
       "      <td>-60.922640</td>\n",
       "      <td>-64.676879</td>\n",
       "      <td>-64.676879</td>\n",
       "      <td>-64.676879</td>\n",
       "      <td>-64.178914</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>-62.691257</td>\n",
       "      <td>-61.921366</td>\n",
       "      <td>-58.112355</td>\n",
       "      <td>-55.378601</td>\n",
       "      <td>-56.545518</td>\n",
       "      <td>-58.246249</td>\n",
       "      <td>-58.498599</td>\n",
       "      <td>-58.577014</td>\n",
       "      <td>-59.843182</td>\n",
       "      <td>-59.149372</td>\n",
       "      <td>...</td>\n",
       "      <td>-56.717537</td>\n",
       "      <td>-58.949967</td>\n",
       "      <td>-57.629906</td>\n",
       "      <td>-59.174890</td>\n",
       "      <td>-58.905429</td>\n",
       "      <td>-57.774437</td>\n",
       "      <td>-63.483848</td>\n",
       "      <td>-65.929731</td>\n",
       "      <td>-64.601276</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>-47.560947</td>\n",
       "      <td>-48.092271</td>\n",
       "      <td>-48.728114</td>\n",
       "      <td>-49.372154</td>\n",
       "      <td>-50.501240</td>\n",
       "      <td>-51.668273</td>\n",
       "      <td>-51.371920</td>\n",
       "      <td>-51.136957</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>...</td>\n",
       "      <td>-51.499885</td>\n",
       "      <td>-51.476879</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>-51.323238</td>\n",
       "      <td>-50.613737</td>\n",
       "      <td>-50.840188</td>\n",
       "      <td>-50.942781</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>-58.867968</td>\n",
       "      <td>-58.666244</td>\n",
       "      <td>-59.207609</td>\n",
       "      <td>-61.005363</td>\n",
       "      <td>-63.547246</td>\n",
       "      <td>-66.702907</td>\n",
       "      <td>-69.242524</td>\n",
       "      <td>-69.242524</td>\n",
       "      <td>-66.966045</td>\n",
       "      <td>-63.217095</td>\n",
       "      <td>...</td>\n",
       "      <td>-62.355711</td>\n",
       "      <td>-61.165540</td>\n",
       "      <td>-61.309433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>-63.396786</td>\n",
       "      <td>-61.330426</td>\n",
       "      <td>-59.724636</td>\n",
       "      <td>-59.734818</td>\n",
       "      <td>-59.192870</td>\n",
       "      <td>-60.495081</td>\n",
       "      <td>-63.579745</td>\n",
       "      <td>-70.879143</td>\n",
       "      <td>-70.885193</td>\n",
       "      <td>-70.885193</td>\n",
       "      <td>...</td>\n",
       "      <td>-59.904442</td>\n",
       "      <td>-65.932897</td>\n",
       "      <td>-68.419112</td>\n",
       "      <td>-66.637226</td>\n",
       "      <td>-63.676880</td>\n",
       "      <td>-59.776530</td>\n",
       "      <td>-59.531735</td>\n",
       "      <td>-65.806624</td>\n",
       "      <td>-65.705298</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>-60.244715</td>\n",
       "      <td>-60.244715</td>\n",
       "      <td>-57.391485</td>\n",
       "      <td>-56.526644</td>\n",
       "      <td>-57.258402</td>\n",
       "      <td>-56.625158</td>\n",
       "      <td>-57.617670</td>\n",
       "      <td>-60.244715</td>\n",
       "      <td>-58.908708</td>\n",
       "      <td>-57.539194</td>\n",
       "      <td>...</td>\n",
       "      <td>-53.254220</td>\n",
       "      <td>-52.002472</td>\n",
       "      <td>-50.719661</td>\n",
       "      <td>-50.529916</td>\n",
       "      <td>-53.913672</td>\n",
       "      <td>-54.701472</td>\n",
       "      <td>-53.199863</td>\n",
       "      <td>-51.693457</td>\n",
       "      <td>-50.263608</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>-59.529203</td>\n",
       "      <td>-58.094118</td>\n",
       "      <td>-58.708832</td>\n",
       "      <td>-60.660041</td>\n",
       "      <td>-59.539054</td>\n",
       "      <td>-58.654146</td>\n",
       "      <td>-57.798042</td>\n",
       "      <td>-56.422977</td>\n",
       "      <td>-56.930666</td>\n",
       "      <td>-59.312142</td>\n",
       "      <td>...</td>\n",
       "      <td>-57.876186</td>\n",
       "      <td>-54.990796</td>\n",
       "      <td>-55.307518</td>\n",
       "      <td>-61.926010</td>\n",
       "      <td>-56.977314</td>\n",
       "      <td>-54.574473</td>\n",
       "      <td>-53.689643</td>\n",
       "      <td>-55.768147</td>\n",
       "      <td>-58.674321</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>-57.419533</td>\n",
       "      <td>-54.810391</td>\n",
       "      <td>-53.672090</td>\n",
       "      <td>-56.541297</td>\n",
       "      <td>-54.398777</td>\n",
       "      <td>-51.947087</td>\n",
       "      <td>-54.291875</td>\n",
       "      <td>-54.611404</td>\n",
       "      <td>-55.476483</td>\n",
       "      <td>-56.366090</td>\n",
       "      <td>...</td>\n",
       "      <td>-58.682303</td>\n",
       "      <td>-60.253011</td>\n",
       "      <td>-59.709635</td>\n",
       "      <td>-58.438877</td>\n",
       "      <td>-59.303890</td>\n",
       "      <td>-63.062989</td>\n",
       "      <td>-65.427534</td>\n",
       "      <td>-65.043220</td>\n",
       "      <td>-65.979849</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5    \\\n",
       "298 -53.756179 -49.120491 -47.523206 -49.048604 -51.008211 -51.015084   \n",
       "299 -48.351829 -47.869963 -46.828234 -49.911606 -50.994150 -47.716347   \n",
       "300 -61.363933 -60.424969 -60.542148 -63.737012 -64.673218 -61.401023   \n",
       "301 -62.691257 -61.921366 -58.112355 -55.378601 -56.545518 -58.246249   \n",
       "302 -47.560947 -48.092271 -48.728114 -49.372154 -50.501240 -51.668273   \n",
       "303 -58.867968 -58.666244 -59.207609 -61.005363 -63.547246 -66.702907   \n",
       "304 -63.396786 -61.330426 -59.724636 -59.734818 -59.192870 -60.495081   \n",
       "305 -60.244715 -60.244715 -57.391485 -56.526644 -57.258402 -56.625158   \n",
       "307 -59.529203 -58.094118 -58.708832 -60.660041 -59.539054 -58.654146   \n",
       "308 -57.419533 -54.810391 -53.672090 -56.541297 -54.398777 -51.947087   \n",
       "\n",
       "           6          7          8          9     ...           207  \\\n",
       "298 -50.169745 -50.886473 -50.275355 -51.907351   ...    -47.518230   \n",
       "299 -47.951298 -50.378521 -49.107214 -50.084609   ...    -28.069463   \n",
       "300 -62.801707 -64.522762 -64.676879 -63.780731   ...    -58.870220   \n",
       "301 -58.498599 -58.577014 -59.843182 -59.149372   ...    -56.717537   \n",
       "302 -51.371920 -51.136957 -51.782069 -51.782069   ...    -51.499885   \n",
       "303 -69.242524 -69.242524 -66.966045 -63.217095   ...    -62.355711   \n",
       "304 -63.579745 -70.879143 -70.885193 -70.885193   ...    -59.904442   \n",
       "305 -57.617670 -60.244715 -58.908708 -57.539194   ...    -53.254220   \n",
       "307 -57.798042 -56.422977 -56.930666 -59.312142   ...    -57.876186   \n",
       "308 -54.291875 -54.611404 -55.476483 -56.366090   ...    -58.682303   \n",
       "\n",
       "           208        209        210        211        212        213  \\\n",
       "298 -46.052274 -45.574740 -46.517232 -48.663554 -49.160694 -51.180327   \n",
       "299 -27.181224 -28.187199 -28.794791 -28.706694 -30.295817 -31.448918   \n",
       "300 -61.683645 -59.392167 -58.266269 -60.922640 -64.676879 -64.676879   \n",
       "301 -58.949967 -57.629906 -59.174890 -58.905429 -57.774437 -63.483848   \n",
       "302 -51.476879 -51.782069 -51.782069 -51.782069 -51.323238 -50.613737   \n",
       "303 -61.165540 -61.309433        NaN        NaN        NaN        NaN   \n",
       "304 -65.932897 -68.419112 -66.637226 -63.676880 -59.776530 -59.531735   \n",
       "305 -52.002472 -50.719661 -50.529916 -53.913672 -54.701472 -53.199863   \n",
       "307 -54.990796 -55.307518 -61.926010 -56.977314 -54.574473 -53.689643   \n",
       "308 -60.253011 -59.709635 -58.438877 -59.303890 -63.062989 -65.427534   \n",
       "\n",
       "           214        215      0    \n",
       "298 -47.262994 -45.473499  disgust  \n",
       "299 -29.981349 -29.475319  disgust  \n",
       "300 -64.676879 -64.178914  fearful  \n",
       "301 -65.929731 -64.601276      sad  \n",
       "302 -50.840188 -50.942781  fearful  \n",
       "303        NaN        NaN     calm  \n",
       "304 -65.806624 -65.705298     calm  \n",
       "305 -51.693457 -50.263608    happy  \n",
       "307 -55.768147 -58.674321  disgust  \n",
       "308 -65.043220 -65.979849      sad  \n",
       "\n",
       "[10 rows x 217 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[250:260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-53.756179</td>\n",
       "      <td>-49.120491</td>\n",
       "      <td>-47.523206</td>\n",
       "      <td>-49.048604</td>\n",
       "      <td>-51.008211</td>\n",
       "      <td>-51.015084</td>\n",
       "      <td>-50.169745</td>\n",
       "      <td>-50.886473</td>\n",
       "      <td>-50.275355</td>\n",
       "      <td>-51.907351</td>\n",
       "      <td>...</td>\n",
       "      <td>-47.518230</td>\n",
       "      <td>-46.052274</td>\n",
       "      <td>-45.574740</td>\n",
       "      <td>-46.517232</td>\n",
       "      <td>-48.663554</td>\n",
       "      <td>-49.160694</td>\n",
       "      <td>-51.180327</td>\n",
       "      <td>-47.262994</td>\n",
       "      <td>-45.473499</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-48.351829</td>\n",
       "      <td>-47.869963</td>\n",
       "      <td>-46.828234</td>\n",
       "      <td>-49.911606</td>\n",
       "      <td>-50.994150</td>\n",
       "      <td>-47.716347</td>\n",
       "      <td>-47.951298</td>\n",
       "      <td>-50.378521</td>\n",
       "      <td>-49.107214</td>\n",
       "      <td>-50.084609</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.069463</td>\n",
       "      <td>-27.181224</td>\n",
       "      <td>-28.187199</td>\n",
       "      <td>-28.794791</td>\n",
       "      <td>-28.706694</td>\n",
       "      <td>-30.295817</td>\n",
       "      <td>-31.448918</td>\n",
       "      <td>-29.981349</td>\n",
       "      <td>-29.475319</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-61.363933</td>\n",
       "      <td>-60.424969</td>\n",
       "      <td>-60.542148</td>\n",
       "      <td>-63.737012</td>\n",
       "      <td>-64.673218</td>\n",
       "      <td>-61.401023</td>\n",
       "      <td>-62.801707</td>\n",
       "      <td>-64.522762</td>\n",
       "      <td>-64.676879</td>\n",
       "      <td>-63.780731</td>\n",
       "      <td>...</td>\n",
       "      <td>-58.870220</td>\n",
       "      <td>-61.683645</td>\n",
       "      <td>-59.392167</td>\n",
       "      <td>-58.266269</td>\n",
       "      <td>-60.922640</td>\n",
       "      <td>-64.676879</td>\n",
       "      <td>-64.676879</td>\n",
       "      <td>-64.676879</td>\n",
       "      <td>-64.178914</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>-62.691257</td>\n",
       "      <td>-61.921366</td>\n",
       "      <td>-58.112355</td>\n",
       "      <td>-55.378601</td>\n",
       "      <td>-56.545518</td>\n",
       "      <td>-58.246249</td>\n",
       "      <td>-58.498599</td>\n",
       "      <td>-58.577014</td>\n",
       "      <td>-59.843182</td>\n",
       "      <td>-59.149372</td>\n",
       "      <td>...</td>\n",
       "      <td>-56.717537</td>\n",
       "      <td>-58.949967</td>\n",
       "      <td>-57.629906</td>\n",
       "      <td>-59.174890</td>\n",
       "      <td>-58.905429</td>\n",
       "      <td>-57.774437</td>\n",
       "      <td>-63.483848</td>\n",
       "      <td>-65.929731</td>\n",
       "      <td>-64.601276</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>-47.560947</td>\n",
       "      <td>-48.092271</td>\n",
       "      <td>-48.728114</td>\n",
       "      <td>-49.372154</td>\n",
       "      <td>-50.501240</td>\n",
       "      <td>-51.668273</td>\n",
       "      <td>-51.371920</td>\n",
       "      <td>-51.136957</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>...</td>\n",
       "      <td>-51.499885</td>\n",
       "      <td>-51.476879</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>-51.323238</td>\n",
       "      <td>-50.613737</td>\n",
       "      <td>-50.840188</td>\n",
       "      <td>-50.942781</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>-58.867968</td>\n",
       "      <td>-58.666244</td>\n",
       "      <td>-59.207609</td>\n",
       "      <td>-61.005363</td>\n",
       "      <td>-63.547246</td>\n",
       "      <td>-66.702907</td>\n",
       "      <td>-69.242524</td>\n",
       "      <td>-69.242524</td>\n",
       "      <td>-66.966045</td>\n",
       "      <td>-63.217095</td>\n",
       "      <td>...</td>\n",
       "      <td>-62.355711</td>\n",
       "      <td>-61.165540</td>\n",
       "      <td>-61.309433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>-63.396786</td>\n",
       "      <td>-61.330426</td>\n",
       "      <td>-59.724636</td>\n",
       "      <td>-59.734818</td>\n",
       "      <td>-59.192870</td>\n",
       "      <td>-60.495081</td>\n",
       "      <td>-63.579745</td>\n",
       "      <td>-70.879143</td>\n",
       "      <td>-70.885193</td>\n",
       "      <td>-70.885193</td>\n",
       "      <td>...</td>\n",
       "      <td>-59.904442</td>\n",
       "      <td>-65.932897</td>\n",
       "      <td>-68.419112</td>\n",
       "      <td>-66.637226</td>\n",
       "      <td>-63.676880</td>\n",
       "      <td>-59.776530</td>\n",
       "      <td>-59.531735</td>\n",
       "      <td>-65.806624</td>\n",
       "      <td>-65.705298</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>-60.244715</td>\n",
       "      <td>-60.244715</td>\n",
       "      <td>-57.391485</td>\n",
       "      <td>-56.526644</td>\n",
       "      <td>-57.258402</td>\n",
       "      <td>-56.625158</td>\n",
       "      <td>-57.617670</td>\n",
       "      <td>-60.244715</td>\n",
       "      <td>-58.908708</td>\n",
       "      <td>-57.539194</td>\n",
       "      <td>...</td>\n",
       "      <td>-53.254220</td>\n",
       "      <td>-52.002472</td>\n",
       "      <td>-50.719661</td>\n",
       "      <td>-50.529916</td>\n",
       "      <td>-53.913672</td>\n",
       "      <td>-54.701472</td>\n",
       "      <td>-53.199863</td>\n",
       "      <td>-51.693457</td>\n",
       "      <td>-50.263608</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>-59.529203</td>\n",
       "      <td>-58.094118</td>\n",
       "      <td>-58.708832</td>\n",
       "      <td>-60.660041</td>\n",
       "      <td>-59.539054</td>\n",
       "      <td>-58.654146</td>\n",
       "      <td>-57.798042</td>\n",
       "      <td>-56.422977</td>\n",
       "      <td>-56.930666</td>\n",
       "      <td>-59.312142</td>\n",
       "      <td>...</td>\n",
       "      <td>-57.876186</td>\n",
       "      <td>-54.990796</td>\n",
       "      <td>-55.307518</td>\n",
       "      <td>-61.926010</td>\n",
       "      <td>-56.977314</td>\n",
       "      <td>-54.574473</td>\n",
       "      <td>-53.689643</td>\n",
       "      <td>-55.768147</td>\n",
       "      <td>-58.674321</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>-57.419533</td>\n",
       "      <td>-54.810391</td>\n",
       "      <td>-53.672090</td>\n",
       "      <td>-56.541297</td>\n",
       "      <td>-54.398777</td>\n",
       "      <td>-51.947087</td>\n",
       "      <td>-54.291875</td>\n",
       "      <td>-54.611404</td>\n",
       "      <td>-55.476483</td>\n",
       "      <td>-56.366090</td>\n",
       "      <td>...</td>\n",
       "      <td>-58.682303</td>\n",
       "      <td>-60.253011</td>\n",
       "      <td>-59.709635</td>\n",
       "      <td>-58.438877</td>\n",
       "      <td>-59.303890</td>\n",
       "      <td>-63.062989</td>\n",
       "      <td>-65.427534</td>\n",
       "      <td>-65.043220</td>\n",
       "      <td>-65.979849</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5    \\\n",
       "298 -53.756179 -49.120491 -47.523206 -49.048604 -51.008211 -51.015084   \n",
       "299 -48.351829 -47.869963 -46.828234 -49.911606 -50.994150 -47.716347   \n",
       "300 -61.363933 -60.424969 -60.542148 -63.737012 -64.673218 -61.401023   \n",
       "301 -62.691257 -61.921366 -58.112355 -55.378601 -56.545518 -58.246249   \n",
       "302 -47.560947 -48.092271 -48.728114 -49.372154 -50.501240 -51.668273   \n",
       "303 -58.867968 -58.666244 -59.207609 -61.005363 -63.547246 -66.702907   \n",
       "304 -63.396786 -61.330426 -59.724636 -59.734818 -59.192870 -60.495081   \n",
       "305 -60.244715 -60.244715 -57.391485 -56.526644 -57.258402 -56.625158   \n",
       "307 -59.529203 -58.094118 -58.708832 -60.660041 -59.539054 -58.654146   \n",
       "308 -57.419533 -54.810391 -53.672090 -56.541297 -54.398777 -51.947087   \n",
       "\n",
       "           6          7          8          9     ...           207  \\\n",
       "298 -50.169745 -50.886473 -50.275355 -51.907351   ...    -47.518230   \n",
       "299 -47.951298 -50.378521 -49.107214 -50.084609   ...    -28.069463   \n",
       "300 -62.801707 -64.522762 -64.676879 -63.780731   ...    -58.870220   \n",
       "301 -58.498599 -58.577014 -59.843182 -59.149372   ...    -56.717537   \n",
       "302 -51.371920 -51.136957 -51.782069 -51.782069   ...    -51.499885   \n",
       "303 -69.242524 -69.242524 -66.966045 -63.217095   ...    -62.355711   \n",
       "304 -63.579745 -70.879143 -70.885193 -70.885193   ...    -59.904442   \n",
       "305 -57.617670 -60.244715 -58.908708 -57.539194   ...    -53.254220   \n",
       "307 -57.798042 -56.422977 -56.930666 -59.312142   ...    -57.876186   \n",
       "308 -54.291875 -54.611404 -55.476483 -56.366090   ...    -58.682303   \n",
       "\n",
       "           208        209        210        211        212        213  \\\n",
       "298 -46.052274 -45.574740 -46.517232 -48.663554 -49.160694 -51.180327   \n",
       "299 -27.181224 -28.187199 -28.794791 -28.706694 -30.295817 -31.448918   \n",
       "300 -61.683645 -59.392167 -58.266269 -60.922640 -64.676879 -64.676879   \n",
       "301 -58.949967 -57.629906 -59.174890 -58.905429 -57.774437 -63.483848   \n",
       "302 -51.476879 -51.782069 -51.782069 -51.782069 -51.323238 -50.613737   \n",
       "303 -61.165540 -61.309433   0.000000   0.000000   0.000000   0.000000   \n",
       "304 -65.932897 -68.419112 -66.637226 -63.676880 -59.776530 -59.531735   \n",
       "305 -52.002472 -50.719661 -50.529916 -53.913672 -54.701472 -53.199863   \n",
       "307 -54.990796 -55.307518 -61.926010 -56.977314 -54.574473 -53.689643   \n",
       "308 -60.253011 -59.709635 -58.438877 -59.303890 -63.062989 -65.427534   \n",
       "\n",
       "           214        215      0    \n",
       "298 -47.262994 -45.473499  disgust  \n",
       "299 -29.981349 -29.475319  disgust  \n",
       "300 -64.676879 -64.178914  fearful  \n",
       "301 -65.929731 -64.601276      sad  \n",
       "302 -50.840188 -50.942781  fearful  \n",
       "303   0.000000   0.000000     calm  \n",
       "304 -65.806624 -65.705298     calm  \n",
       "305 -51.693457 -50.263608    happy  \n",
       "307 -55.768147 -58.674321  disgust  \n",
       "308 -65.043220 -65.979849      sad  \n",
       "\n",
       "[10 rows x 217 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[250:260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfeatures = train.iloc[:, :-1]\n",
    "trainlabel = train.iloc[:, -1:]\n",
    "testfeatures = test.iloc[:, :-1]\n",
    "testlabel = test.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train = np.array(trainfeatures)\n",
    "y_train = np.array(trainlabel)\n",
    "X_test = np.array(testfeatures)\n",
    "y_test = np.array(testlabel)\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = np_utils.to_categorical(lb.fit_transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1171, 216)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_traincnn =np.expand_dims(X_train, axis=2)\n",
    "x_testcnn= np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(128, 5,padding='same',\n",
    "                 input_shape=(256,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_11 (Conv1D)           (None, 216, 256)          1536      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 216, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 216, 128)          163968    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 3, 128)            82048     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 3080      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 414,728\n",
      "Trainable params: 414,728\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv1d_18_input to have shape (None, 256, 1) but got array with shape (1171, 216, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-6b22d19cb7db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnnhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_traincnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_testcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1582\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1412\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1415\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1416\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    151\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv1d_18_input to have shape (None, 256, 1) but got array with shape (1171, 216, 1)"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "n_classes = 1\n",
    "n_units = 200\n",
    "n_features = 216\n",
    "batch_size = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "xplaceholder= tf.placeholder('float',[None,n_features])\n",
    "yplaceholder = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'split_3:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:1' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:2' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:3' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:4' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:5' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:6' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:7' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:8' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:9' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:10' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:11' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:12' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:13' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:14' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:15' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:16' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:17' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:18' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:19' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:20' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:21' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:22' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:23' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:24' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:25' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:26' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:27' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:28' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:29' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:30' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:31' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:32' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:33' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:34' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:35' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:36' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:37' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:38' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:39' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:40' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:41' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:42' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:43' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:44' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:45' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:46' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:47' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:48' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:49' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:50' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:51' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:52' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:53' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:54' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:55' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:56' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:57' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:58' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:59' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:60' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:61' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:62' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:63' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:64' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:65' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:66' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:67' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:68' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:69' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:70' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:71' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:72' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:73' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:74' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:75' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:76' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:77' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:78' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:79' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:80' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:81' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:82' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:83' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:84' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:85' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:86' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:87' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:88' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:89' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:90' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:91' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:92' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:93' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:94' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:95' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:96' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:97' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:98' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:99' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:100' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:101' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:102' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:103' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:104' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:105' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:106' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:107' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:108' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:109' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:110' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:111' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:112' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:113' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:114' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:115' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:116' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:117' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:118' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:119' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:120' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:121' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:122' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:123' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:124' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:125' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:126' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:127' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:128' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:129' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:130' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:131' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:132' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:133' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:134' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:135' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:136' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:137' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:138' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:139' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:140' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:141' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:142' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:143' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:144' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:145' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:146' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:147' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:148' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:149' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:150' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:151' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:152' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:153' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:154' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:155' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:156' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:157' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:158' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:159' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:160' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:161' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:162' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:163' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:164' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:165' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:166' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:167' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:168' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:169' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:170' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:171' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:172' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:173' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:174' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:175' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:176' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:177' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:178' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:179' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:180' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:181' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:182' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:183' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:184' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:185' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:186' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:187' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:188' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:189' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:190' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:191' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:192' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:193' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:194' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:195' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:196' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:197' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:198' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:199' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:200' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:201' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:202' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:203' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:204' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:205' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:206' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:207' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:208' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:209' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:210' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:211' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:212' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:213' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:214' shape=(?, 1) dtype=float32>, <tf.Tensor 'split_3:215' shape=(?, 1) dtype=float32>]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable rnn/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-90-2c0eb9a60215>\", line 16, in recurrent_neural_network_model\n    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n  File \"<ipython-input-90-2c0eb9a60215>\", line 23, in train_neural_network\n    logit = recurrent_neural_network_model()\n  File \"<ipython-input-90-2c0eb9a60215>\", line 63, in <module>\n    train_neural_network()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-2c0eb9a60215>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-94-2c0eb9a60215>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecurrent_neural_network_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-94-2c0eb9a60215>\u001b[0m in \u001b[0;36mrecurrent_neural_network_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlstm_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mstatic_rnn\u001b[0;34m(cell, inputs, initial_state, dtype, sequence_length, scope)\u001b[0m\n\u001b[1;32m   1210\u001b[0m             state_size=cell.state_size)\n\u001b[1;32m   1211\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m       \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mvarscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m       \u001b[0;31m# pylint: disable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m       \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m       \u001b[0;31m# pylint: enable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    178\u001b[0m       with vs.variable_scope(vs.get_variable_scope(),\n\u001b[1;32m    179\u001b[0m                              custom_getter=self._rnn_get_variable):\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;31m# Apply activity regularization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m    381\u001b[0m       \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_or_size_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m     \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;31m# i = input_gate, j = new_input, f = forget_gate, o = output_gate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m_linear\u001b[0;34m(args, output_size, bias, bias_initializer, kernel_initializer)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0m_WEIGHTS_VARIABLE_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtotal_arg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m         initializer=kernel_initializer)\n\u001b[0m\u001b[1;32m   1018\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m       \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1063\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1066\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1067\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    960\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    358\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    361\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m       return _true_getter(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m_rnn_get_variable\u001b[0;34m(self, getter, *args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m     trainable = (variable in tf_variables.trainable_variables() or\n\u001b[1;32m    185\u001b[0m                  (isinstance(variable, tf_variables.PartitionedVariable) and\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    350\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    662\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 664\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    665\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable rnn/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-90-2c0eb9a60215>\", line 16, in recurrent_neural_network_model\n    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n  File \"<ipython-input-90-2c0eb9a60215>\", line 23, in train_neural_network\n    logit = recurrent_neural_network_model()\n  File \"<ipython-input-90-2c0eb9a60215>\", line 63, in <module>\n    train_neural_network()\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "\n",
    "def recurrent_neural_network_model():\n",
    "    layer ={ 'weights': tf.Variable(tf.random_normal([n_units, n_classes])),'bias': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    x = tf.split(xplaceholder, n_features, 1)\n",
    "    print(x)\n",
    "\n",
    "    lstm_cell = rnn.BasicLSTMCell(n_units)\n",
    "    \n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "   \n",
    "    output = tf.matmul(outputs[-1], layer['weights']) + layer['bias']\n",
    "\n",
    "    return output\n",
    "\n",
    "def train_neural_network():\n",
    "    logit = recurrent_neural_network_model()\n",
    "    logit = tf.reshape(logit, [-1])\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=yplaceholder))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            i = 0\n",
    "            for i in range(int(len(X_train) / batch_size)):\n",
    "\n",
    "                start = i\n",
    "                end = i + batch_size\n",
    "\n",
    "                batch_x = np.array(X_train[start:end])\n",
    "                batch_y = np.array(y_train[start:end])\n",
    "                \n",
    "                _, c = sess.run([optimizer, cost], feed_dict={xplaceholder: batch_x, yplaceholder: batch_y})\n",
    "                epoch_loss += c\n",
    "                i += batch_size\n",
    "\n",
    "            print('Epoch', epoch, 'completed out of', epochs, 'loss:', epoch_loss)\n",
    "\n",
    "        pred = tf.round(tf.nn.sigmoid(logit)).eval({xplaceholder: np.array(X_test), yplaceholder: np.array(y_test)})\n",
    "        f1 = f1_score(np.array(y_test), pred, average='macro')\n",
    "        accuracy=accuracy_score(np.array(y_test), pred)\n",
    "        recall = recall_score(y_true=np.array(y_test), y_pred= pred)\n",
    "        precision = precision_score(y_true=np.array(y_test), y_pred=pred)\n",
    "        print(\"F1 Score:\", f1)\n",
    "        print(\"Accuracy Score:\",accuracy)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"Precision:\", precision)\n",
    "\n",
    "\n",
    "train_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
