{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa import display\n",
    "import pandas as pd\n",
    "import glob \n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Dataset/Audio_Speech_Actors_01-24\"\n",
    "feeling_list = []\n",
    "for root,dirs,files in os.walk(path):\n",
    "    if(len(files)>0):\n",
    "        for file in files:\n",
    "            if(int(file.split(\".\")[0].split(\"-\")[2]) == 1):\n",
    "                feeling_list.append('neutral')\n",
    "            elif(int(file.split(\".\")[0].split(\"-\")[2]) == 2):\n",
    "                feeling_list.append('calm')\n",
    "            elif(int(file.split(\".\")[0].split(\"-\")[2]) == 3):\n",
    "                feeling_list.append('happy')\n",
    "            elif(int(file.split(\".\")[0].split(\"-\")[2]) == 4):\n",
    "                feeling_list.append('sad')\n",
    "            elif(int(file.split(\".\")[0].split(\"-\")[2]) == 5):\n",
    "                feeling_list.append('angry')\n",
    "            elif(int(file.split(\".\")[0].split(\"-\")[2]) == 6):\n",
    "                feeling_list.append('fearful')\n",
    "            elif(int(file.split(\".\")[0].split(\"-\")[2]) == 7):\n",
    "                feeling_list.append('disgust')\n",
    "            elif(int(file.split(\".\")[0].split(\"-\")[2]) == 8):\n",
    "                feeling_list.append('surprised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "0        sad\n",
       "1        sad\n",
       "2      happy\n",
       "3  surprised\n",
       "4      happy\n",
       "5       calm\n",
       "6       calm\n",
       "7      angry\n",
       "8    neutral\n",
       "9    fearful"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(feeling_list))\n",
    "labels = pd.DataFrame(feeling_list , columns=['label'])\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark = 0\n",
    "for subdir,dirs,files in os.walk(path):\n",
    "    for file in files:\n",
    "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "        sample_rate = np.array(sample_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                            sr=sample_rate, \n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark=bookmark+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-71.80572946267719, -71.80572946267719, -71.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-57.983965439717444, -58.12975730353416, -57....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-51.123276362992605, -51.8187817185881, -51.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-59.1965370795561, -59.1965370795561, -59.196...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-43.633354817987765, -45.07731733923093, -47....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature\n",
       "0  [-71.80572946267719, -71.80572946267719, -71.8...\n",
       "1  [-57.983965439717444, -58.12975730353416, -57....\n",
       "2  [-51.123276362992605, -51.8187817185881, -51.9...\n",
       "3  [-59.1965370795561, -59.1965370795561, -59.196...\n",
       "4  [-43.633354817987765, -45.07731733923093, -47...."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(df['feature'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.concat([df3,labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>-71.805729</td>\n",
       "      <td>...</td>\n",
       "      <td>-48.080450</td>\n",
       "      <td>-48.708313</td>\n",
       "      <td>-47.572926</td>\n",
       "      <td>-48.468573</td>\n",
       "      <td>-48.897654</td>\n",
       "      <td>-50.247077</td>\n",
       "      <td>-49.038084</td>\n",
       "      <td>-48.715561</td>\n",
       "      <td>-51.587351</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-57.983965</td>\n",
       "      <td>-58.129757</td>\n",
       "      <td>-57.216111</td>\n",
       "      <td>-57.138616</td>\n",
       "      <td>-58.032113</td>\n",
       "      <td>-57.584437</td>\n",
       "      <td>-57.990391</td>\n",
       "      <td>-57.497483</td>\n",
       "      <td>-57.414894</td>\n",
       "      <td>-58.088535</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.392082</td>\n",
       "      <td>-40.890291</td>\n",
       "      <td>-42.336407</td>\n",
       "      <td>-43.509533</td>\n",
       "      <td>-42.348652</td>\n",
       "      <td>-42.181487</td>\n",
       "      <td>-45.076798</td>\n",
       "      <td>-45.154829</td>\n",
       "      <td>-39.698874</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-51.123276</td>\n",
       "      <td>-51.818782</td>\n",
       "      <td>-51.941592</td>\n",
       "      <td>-52.816664</td>\n",
       "      <td>-54.652136</td>\n",
       "      <td>-56.915060</td>\n",
       "      <td>-57.323525</td>\n",
       "      <td>-56.916718</td>\n",
       "      <td>-54.595758</td>\n",
       "      <td>-54.187025</td>\n",
       "      <td>...</td>\n",
       "      <td>-50.799301</td>\n",
       "      <td>-50.775673</td>\n",
       "      <td>-51.096929</td>\n",
       "      <td>-51.170862</td>\n",
       "      <td>-50.121700</td>\n",
       "      <td>-49.047896</td>\n",
       "      <td>-49.425883</td>\n",
       "      <td>-50.382450</td>\n",
       "      <td>-53.368919</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>-59.196537</td>\n",
       "      <td>...</td>\n",
       "      <td>-48.255887</td>\n",
       "      <td>-48.819326</td>\n",
       "      <td>-49.072596</td>\n",
       "      <td>-49.837795</td>\n",
       "      <td>-50.631560</td>\n",
       "      <td>-49.627614</td>\n",
       "      <td>-49.795650</td>\n",
       "      <td>-49.463476</td>\n",
       "      <td>-49.567603</td>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.633355</td>\n",
       "      <td>-45.077317</td>\n",
       "      <td>-47.110537</td>\n",
       "      <td>-47.066368</td>\n",
       "      <td>-46.836029</td>\n",
       "      <td>-46.536989</td>\n",
       "      <td>-47.593565</td>\n",
       "      <td>-47.908471</td>\n",
       "      <td>-48.006105</td>\n",
       "      <td>-47.703133</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.769527</td>\n",
       "      <td>-16.043667</td>\n",
       "      <td>-14.923637</td>\n",
       "      <td>-14.839569</td>\n",
       "      <td>-15.081686</td>\n",
       "      <td>-17.688219</td>\n",
       "      <td>-18.029030</td>\n",
       "      <td>-17.959716</td>\n",
       "      <td>-18.121766</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5  \\\n",
       "0 -71.805729 -71.805729 -71.805729 -71.805729 -71.805729 -71.805729   \n",
       "1 -57.983965 -58.129757 -57.216111 -57.138616 -58.032113 -57.584437   \n",
       "2 -51.123276 -51.818782 -51.941592 -52.816664 -54.652136 -56.915060   \n",
       "3 -59.196537 -59.196537 -59.196537 -59.196537 -59.196537 -59.196537   \n",
       "4 -43.633355 -45.077317 -47.110537 -47.066368 -46.836029 -46.536989   \n",
       "\n",
       "           6          7          8          9    ...            207  \\\n",
       "0 -71.805729 -71.805729 -71.805729 -71.805729    ...     -48.080450   \n",
       "1 -57.990391 -57.497483 -57.414894 -58.088535    ...     -39.392082   \n",
       "2 -57.323525 -56.916718 -54.595758 -54.187025    ...     -50.799301   \n",
       "3 -59.196537 -59.196537 -59.196537 -59.196537    ...     -48.255887   \n",
       "4 -47.593565 -47.908471 -48.006105 -47.703133    ...     -17.769527   \n",
       "\n",
       "         208        209        210        211        212        213  \\\n",
       "0 -48.708313 -47.572926 -48.468573 -48.897654 -50.247077 -49.038084   \n",
       "1 -40.890291 -42.336407 -43.509533 -42.348652 -42.181487 -45.076798   \n",
       "2 -50.775673 -51.096929 -51.170862 -50.121700 -49.047896 -49.425883   \n",
       "3 -48.819326 -49.072596 -49.837795 -50.631560 -49.627614 -49.795650   \n",
       "4 -16.043667 -14.923637 -14.839569 -15.081686 -17.688219 -18.029030   \n",
       "\n",
       "         214        215      label  \n",
       "0 -48.715561 -51.587351        sad  \n",
       "1 -45.154829 -39.698874        sad  \n",
       "2 -50.382450 -53.368919      happy  \n",
       "3 -49.463476 -49.567603  surprised  \n",
       "4 -17.959716 -18.121766      happy  \n",
       "\n",
       "[5 rows x 217 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.to_csv(\"mfccFeatures_emotion_latest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>-54.511486</td>\n",
       "      <td>-53.330406</td>\n",
       "      <td>-52.614726</td>\n",
       "      <td>-55.296237</td>\n",
       "      <td>-58.634653</td>\n",
       "      <td>-58.468420</td>\n",
       "      <td>-56.901425</td>\n",
       "      <td>-57.517333</td>\n",
       "      <td>-55.310247</td>\n",
       "      <td>-51.296041</td>\n",
       "      <td>...</td>\n",
       "      <td>-44.802806</td>\n",
       "      <td>-44.482723</td>\n",
       "      <td>-43.834654</td>\n",
       "      <td>-45.097868</td>\n",
       "      <td>-46.659864</td>\n",
       "      <td>-46.247735</td>\n",
       "      <td>-47.877711</td>\n",
       "      <td>-47.970408</td>\n",
       "      <td>-44.807865</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>-56.357233</td>\n",
       "      <td>-55.912082</td>\n",
       "      <td>-53.390019</td>\n",
       "      <td>-54.204491</td>\n",
       "      <td>-51.211485</td>\n",
       "      <td>-51.550976</td>\n",
       "      <td>-50.826821</td>\n",
       "      <td>-50.699000</td>\n",
       "      <td>-51.794851</td>\n",
       "      <td>-52.526187</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.346369</td>\n",
       "      <td>-52.856998</td>\n",
       "      <td>-52.708602</td>\n",
       "      <td>-52.077265</td>\n",
       "      <td>-50.171066</td>\n",
       "      <td>-49.525395</td>\n",
       "      <td>-50.240926</td>\n",
       "      <td>-52.254960</td>\n",
       "      <td>-54.420155</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>-41.033862</td>\n",
       "      <td>-43.473726</td>\n",
       "      <td>-44.572001</td>\n",
       "      <td>-45.404978</td>\n",
       "      <td>-45.459836</td>\n",
       "      <td>-45.979375</td>\n",
       "      <td>-46.638765</td>\n",
       "      <td>-45.706508</td>\n",
       "      <td>-45.103673</td>\n",
       "      <td>-45.049784</td>\n",
       "      <td>...</td>\n",
       "      <td>-36.153344</td>\n",
       "      <td>-35.001036</td>\n",
       "      <td>-35.734099</td>\n",
       "      <td>-34.929305</td>\n",
       "      <td>-33.435040</td>\n",
       "      <td>-32.902015</td>\n",
       "      <td>-32.227553</td>\n",
       "      <td>-32.627929</td>\n",
       "      <td>-30.547536</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-58.545328</td>\n",
       "      <td>-59.202478</td>\n",
       "      <td>-60.137033</td>\n",
       "      <td>-62.037020</td>\n",
       "      <td>-62.147955</td>\n",
       "      <td>-61.235847</td>\n",
       "      <td>-61.505283</td>\n",
       "      <td>-61.574950</td>\n",
       "      <td>-61.505381</td>\n",
       "      <td>-60.495135</td>\n",
       "      <td>...</td>\n",
       "      <td>-62.534837</td>\n",
       "      <td>-63.489974</td>\n",
       "      <td>-63.011312</td>\n",
       "      <td>-60.754591</td>\n",
       "      <td>-60.942248</td>\n",
       "      <td>-61.236054</td>\n",
       "      <td>-61.601959</td>\n",
       "      <td>-60.400122</td>\n",
       "      <td>-60.975515</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>-49.499418</td>\n",
       "      <td>-48.170479</td>\n",
       "      <td>-49.352273</td>\n",
       "      <td>-52.664401</td>\n",
       "      <td>-50.219882</td>\n",
       "      <td>-49.810335</td>\n",
       "      <td>-49.938510</td>\n",
       "      <td>-50.251170</td>\n",
       "      <td>-44.789156</td>\n",
       "      <td>-40.580678</td>\n",
       "      <td>...</td>\n",
       "      <td>-47.737491</td>\n",
       "      <td>-49.919034</td>\n",
       "      <td>-49.157083</td>\n",
       "      <td>-48.099718</td>\n",
       "      <td>-48.619908</td>\n",
       "      <td>-49.115151</td>\n",
       "      <td>-47.950252</td>\n",
       "      <td>-48.969603</td>\n",
       "      <td>-49.690079</td>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>-53.678315</td>\n",
       "      <td>-54.377914</td>\n",
       "      <td>-55.140844</td>\n",
       "      <td>-56.300240</td>\n",
       "      <td>-55.326932</td>\n",
       "      <td>-55.119810</td>\n",
       "      <td>-56.282534</td>\n",
       "      <td>-54.970338</td>\n",
       "      <td>-53.334915</td>\n",
       "      <td>-52.519249</td>\n",
       "      <td>...</td>\n",
       "      <td>-50.354847</td>\n",
       "      <td>-51.740110</td>\n",
       "      <td>-53.151655</td>\n",
       "      <td>-53.596402</td>\n",
       "      <td>-53.313462</td>\n",
       "      <td>-53.512526</td>\n",
       "      <td>-55.573837</td>\n",
       "      <td>-56.696713</td>\n",
       "      <td>-56.354921</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-64.968548</td>\n",
       "      <td>-67.252646</td>\n",
       "      <td>...</td>\n",
       "      <td>-64.678076</td>\n",
       "      <td>-64.360786</td>\n",
       "      <td>-64.642809</td>\n",
       "      <td>-62.274782</td>\n",
       "      <td>-62.417866</td>\n",
       "      <td>-63.560128</td>\n",
       "      <td>-63.105106</td>\n",
       "      <td>-63.359077</td>\n",
       "      <td>-63.284678</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>-59.376024</td>\n",
       "      <td>...</td>\n",
       "      <td>-46.490646</td>\n",
       "      <td>-46.694752</td>\n",
       "      <td>-48.137768</td>\n",
       "      <td>-47.240571</td>\n",
       "      <td>-47.443421</td>\n",
       "      <td>-45.964384</td>\n",
       "      <td>-47.454131</td>\n",
       "      <td>-44.490975</td>\n",
       "      <td>-44.662811</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>-70.152745</td>\n",
       "      <td>-69.913691</td>\n",
       "      <td>-67.810702</td>\n",
       "      <td>-68.664977</td>\n",
       "      <td>-71.470452</td>\n",
       "      <td>-71.470452</td>\n",
       "      <td>-71.470452</td>\n",
       "      <td>-71.470452</td>\n",
       "      <td>-71.470452</td>\n",
       "      <td>-71.470452</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.779465</td>\n",
       "      <td>-43.768887</td>\n",
       "      <td>-44.085496</td>\n",
       "      <td>-43.975650</td>\n",
       "      <td>-44.347019</td>\n",
       "      <td>-44.098364</td>\n",
       "      <td>-46.298862</td>\n",
       "      <td>-47.547014</td>\n",
       "      <td>-46.484299</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>-55.243801</td>\n",
       "      <td>-56.378484</td>\n",
       "      <td>-57.449357</td>\n",
       "      <td>-57.225963</td>\n",
       "      <td>-57.018199</td>\n",
       "      <td>-55.223716</td>\n",
       "      <td>-53.239328</td>\n",
       "      <td>-52.637030</td>\n",
       "      <td>-52.636234</td>\n",
       "      <td>-53.478285</td>\n",
       "      <td>...</td>\n",
       "      <td>-35.352025</td>\n",
       "      <td>-34.598943</td>\n",
       "      <td>-34.833487</td>\n",
       "      <td>-32.159613</td>\n",
       "      <td>-30.491879</td>\n",
       "      <td>-31.179180</td>\n",
       "      <td>-29.126147</td>\n",
       "      <td>-20.525918</td>\n",
       "      <td>-15.224331</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1          2          3          4          5  \\\n",
       "652  -54.511486 -53.330406 -52.614726 -55.296237 -58.634653 -58.468420   \n",
       "524  -56.357233 -55.912082 -53.390019 -54.204491 -51.211485 -51.550976   \n",
       "470  -41.033862 -43.473726 -44.572001 -45.404978 -45.459836 -45.979375   \n",
       "11   -58.545328 -59.202478 -60.137033 -62.037020 -62.147955 -61.235847   \n",
       "277  -49.499418 -48.170479 -49.352273 -52.664401 -50.219882 -49.810335   \n",
       "621  -53.678315 -54.377914 -55.140844 -56.300240 -55.326932 -55.119810   \n",
       "773  -64.968548 -64.968548 -64.968548 -64.968548 -64.968548 -64.968548   \n",
       "1357 -59.376024 -59.376024 -59.376024 -59.376024 -59.376024 -59.376024   \n",
       "170  -70.152745 -69.913691 -67.810702 -68.664977 -71.470452 -71.470452   \n",
       "1174 -55.243801 -56.378484 -57.449357 -57.225963 -57.018199 -55.223716   \n",
       "\n",
       "              6          7          8          9    ...            207  \\\n",
       "652  -56.901425 -57.517333 -55.310247 -51.296041    ...     -44.802806   \n",
       "524  -50.826821 -50.699000 -51.794851 -52.526187    ...     -52.346369   \n",
       "470  -46.638765 -45.706508 -45.103673 -45.049784    ...     -36.153344   \n",
       "11   -61.505283 -61.574950 -61.505381 -60.495135    ...     -62.534837   \n",
       "277  -49.938510 -50.251170 -44.789156 -40.580678    ...     -47.737491   \n",
       "621  -56.282534 -54.970338 -53.334915 -52.519249    ...     -50.354847   \n",
       "773  -64.968548 -64.968548 -64.968548 -67.252646    ...     -64.678076   \n",
       "1357 -59.376024 -59.376024 -59.376024 -59.376024    ...     -46.490646   \n",
       "170  -71.470452 -71.470452 -71.470452 -71.470452    ...     -43.779465   \n",
       "1174 -53.239328 -52.637030 -52.636234 -53.478285    ...     -35.352025   \n",
       "\n",
       "            208        209        210        211        212        213  \\\n",
       "652  -44.482723 -43.834654 -45.097868 -46.659864 -46.247735 -47.877711   \n",
       "524  -52.856998 -52.708602 -52.077265 -50.171066 -49.525395 -50.240926   \n",
       "470  -35.001036 -35.734099 -34.929305 -33.435040 -32.902015 -32.227553   \n",
       "11   -63.489974 -63.011312 -60.754591 -60.942248 -61.236054 -61.601959   \n",
       "277  -49.919034 -49.157083 -48.099718 -48.619908 -49.115151 -47.950252   \n",
       "621  -51.740110 -53.151655 -53.596402 -53.313462 -53.512526 -55.573837   \n",
       "773  -64.360786 -64.642809 -62.274782 -62.417866 -63.560128 -63.105106   \n",
       "1357 -46.694752 -48.137768 -47.240571 -47.443421 -45.964384 -47.454131   \n",
       "170  -43.768887 -44.085496 -43.975650 -44.347019 -44.098364 -46.298862   \n",
       "1174 -34.598943 -34.833487 -32.159613 -30.491879 -31.179180 -29.126147   \n",
       "\n",
       "            214        215      label  \n",
       "652  -47.970408 -44.807865        sad  \n",
       "524  -52.254960 -54.420155      happy  \n",
       "470  -32.627929 -30.547536    fearful  \n",
       "11   -60.400122 -60.975515    fearful  \n",
       "277  -48.969603 -49.690079  surprised  \n",
       "621  -56.696713 -56.354921      angry  \n",
       "773  -63.359077 -63.284678      happy  \n",
       "1357 -44.490975 -44.662811    disgust  \n",
       "170  -47.547014 -46.484299       calm  \n",
       "1174 -20.525918 -15.224331    disgust  \n",
       "\n",
       "[10 rows x 217 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "newdf = shuffle(newdf)\n",
    "newdf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf=newdf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf1 = np.random.rand(len(rnewdf)) < 0.8\n",
    "train = rnewdf[newdf1]\n",
    "test = rnewdf[~newdf1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-53.756179</td>\n",
       "      <td>-49.120491</td>\n",
       "      <td>-47.523206</td>\n",
       "      <td>-49.048604</td>\n",
       "      <td>-51.008211</td>\n",
       "      <td>-51.015084</td>\n",
       "      <td>-50.169745</td>\n",
       "      <td>-50.886473</td>\n",
       "      <td>-50.275355</td>\n",
       "      <td>-51.907351</td>\n",
       "      <td>...</td>\n",
       "      <td>-47.518230</td>\n",
       "      <td>-46.052274</td>\n",
       "      <td>-45.574740</td>\n",
       "      <td>-46.517232</td>\n",
       "      <td>-48.663554</td>\n",
       "      <td>-49.160694</td>\n",
       "      <td>-51.180327</td>\n",
       "      <td>-47.262994</td>\n",
       "      <td>-45.473499</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-48.351829</td>\n",
       "      <td>-47.869963</td>\n",
       "      <td>-46.828234</td>\n",
       "      <td>-49.911606</td>\n",
       "      <td>-50.994150</td>\n",
       "      <td>-47.716347</td>\n",
       "      <td>-47.951298</td>\n",
       "      <td>-50.378521</td>\n",
       "      <td>-49.107214</td>\n",
       "      <td>-50.084609</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.069463</td>\n",
       "      <td>-27.181224</td>\n",
       "      <td>-28.187199</td>\n",
       "      <td>-28.794791</td>\n",
       "      <td>-28.706694</td>\n",
       "      <td>-30.295817</td>\n",
       "      <td>-31.448918</td>\n",
       "      <td>-29.981349</td>\n",
       "      <td>-29.475319</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-61.363933</td>\n",
       "      <td>-60.424969</td>\n",
       "      <td>-60.542148</td>\n",
       "      <td>-63.737012</td>\n",
       "      <td>-64.673218</td>\n",
       "      <td>-61.401023</td>\n",
       "      <td>-62.801707</td>\n",
       "      <td>-64.522762</td>\n",
       "      <td>-64.676879</td>\n",
       "      <td>-63.780731</td>\n",
       "      <td>...</td>\n",
       "      <td>-58.870220</td>\n",
       "      <td>-61.683645</td>\n",
       "      <td>-59.392167</td>\n",
       "      <td>-58.266269</td>\n",
       "      <td>-60.922640</td>\n",
       "      <td>-64.676879</td>\n",
       "      <td>-64.676879</td>\n",
       "      <td>-64.676879</td>\n",
       "      <td>-64.178914</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>-62.691257</td>\n",
       "      <td>-61.921366</td>\n",
       "      <td>-58.112355</td>\n",
       "      <td>-55.378601</td>\n",
       "      <td>-56.545518</td>\n",
       "      <td>-58.246249</td>\n",
       "      <td>-58.498599</td>\n",
       "      <td>-58.577014</td>\n",
       "      <td>-59.843182</td>\n",
       "      <td>-59.149372</td>\n",
       "      <td>...</td>\n",
       "      <td>-56.717537</td>\n",
       "      <td>-58.949967</td>\n",
       "      <td>-57.629906</td>\n",
       "      <td>-59.174890</td>\n",
       "      <td>-58.905429</td>\n",
       "      <td>-57.774437</td>\n",
       "      <td>-63.483848</td>\n",
       "      <td>-65.929731</td>\n",
       "      <td>-64.601276</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>-47.560947</td>\n",
       "      <td>-48.092271</td>\n",
       "      <td>-48.728114</td>\n",
       "      <td>-49.372154</td>\n",
       "      <td>-50.501240</td>\n",
       "      <td>-51.668273</td>\n",
       "      <td>-51.371920</td>\n",
       "      <td>-51.136957</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>...</td>\n",
       "      <td>-51.499885</td>\n",
       "      <td>-51.476879</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>-51.323238</td>\n",
       "      <td>-50.613737</td>\n",
       "      <td>-50.840188</td>\n",
       "      <td>-50.942781</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>-58.867968</td>\n",
       "      <td>-58.666244</td>\n",
       "      <td>-59.207609</td>\n",
       "      <td>-61.005363</td>\n",
       "      <td>-63.547246</td>\n",
       "      <td>-66.702907</td>\n",
       "      <td>-69.242524</td>\n",
       "      <td>-69.242524</td>\n",
       "      <td>-66.966045</td>\n",
       "      <td>-63.217095</td>\n",
       "      <td>...</td>\n",
       "      <td>-62.355711</td>\n",
       "      <td>-61.165540</td>\n",
       "      <td>-61.309433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>-63.396786</td>\n",
       "      <td>-61.330426</td>\n",
       "      <td>-59.724636</td>\n",
       "      <td>-59.734818</td>\n",
       "      <td>-59.192870</td>\n",
       "      <td>-60.495081</td>\n",
       "      <td>-63.579745</td>\n",
       "      <td>-70.879143</td>\n",
       "      <td>-70.885193</td>\n",
       "      <td>-70.885193</td>\n",
       "      <td>...</td>\n",
       "      <td>-59.904442</td>\n",
       "      <td>-65.932897</td>\n",
       "      <td>-68.419112</td>\n",
       "      <td>-66.637226</td>\n",
       "      <td>-63.676880</td>\n",
       "      <td>-59.776530</td>\n",
       "      <td>-59.531735</td>\n",
       "      <td>-65.806624</td>\n",
       "      <td>-65.705298</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>-60.244715</td>\n",
       "      <td>-60.244715</td>\n",
       "      <td>-57.391485</td>\n",
       "      <td>-56.526644</td>\n",
       "      <td>-57.258402</td>\n",
       "      <td>-56.625158</td>\n",
       "      <td>-57.617670</td>\n",
       "      <td>-60.244715</td>\n",
       "      <td>-58.908708</td>\n",
       "      <td>-57.539194</td>\n",
       "      <td>...</td>\n",
       "      <td>-53.254220</td>\n",
       "      <td>-52.002472</td>\n",
       "      <td>-50.719661</td>\n",
       "      <td>-50.529916</td>\n",
       "      <td>-53.913672</td>\n",
       "      <td>-54.701472</td>\n",
       "      <td>-53.199863</td>\n",
       "      <td>-51.693457</td>\n",
       "      <td>-50.263608</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>-59.529203</td>\n",
       "      <td>-58.094118</td>\n",
       "      <td>-58.708832</td>\n",
       "      <td>-60.660041</td>\n",
       "      <td>-59.539054</td>\n",
       "      <td>-58.654146</td>\n",
       "      <td>-57.798042</td>\n",
       "      <td>-56.422977</td>\n",
       "      <td>-56.930666</td>\n",
       "      <td>-59.312142</td>\n",
       "      <td>...</td>\n",
       "      <td>-57.876186</td>\n",
       "      <td>-54.990796</td>\n",
       "      <td>-55.307518</td>\n",
       "      <td>-61.926010</td>\n",
       "      <td>-56.977314</td>\n",
       "      <td>-54.574473</td>\n",
       "      <td>-53.689643</td>\n",
       "      <td>-55.768147</td>\n",
       "      <td>-58.674321</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>-57.419533</td>\n",
       "      <td>-54.810391</td>\n",
       "      <td>-53.672090</td>\n",
       "      <td>-56.541297</td>\n",
       "      <td>-54.398777</td>\n",
       "      <td>-51.947087</td>\n",
       "      <td>-54.291875</td>\n",
       "      <td>-54.611404</td>\n",
       "      <td>-55.476483</td>\n",
       "      <td>-56.366090</td>\n",
       "      <td>...</td>\n",
       "      <td>-58.682303</td>\n",
       "      <td>-60.253011</td>\n",
       "      <td>-59.709635</td>\n",
       "      <td>-58.438877</td>\n",
       "      <td>-59.303890</td>\n",
       "      <td>-63.062989</td>\n",
       "      <td>-65.427534</td>\n",
       "      <td>-65.043220</td>\n",
       "      <td>-65.979849</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5    \\\n",
       "298 -53.756179 -49.120491 -47.523206 -49.048604 -51.008211 -51.015084   \n",
       "299 -48.351829 -47.869963 -46.828234 -49.911606 -50.994150 -47.716347   \n",
       "300 -61.363933 -60.424969 -60.542148 -63.737012 -64.673218 -61.401023   \n",
       "301 -62.691257 -61.921366 -58.112355 -55.378601 -56.545518 -58.246249   \n",
       "302 -47.560947 -48.092271 -48.728114 -49.372154 -50.501240 -51.668273   \n",
       "303 -58.867968 -58.666244 -59.207609 -61.005363 -63.547246 -66.702907   \n",
       "304 -63.396786 -61.330426 -59.724636 -59.734818 -59.192870 -60.495081   \n",
       "305 -60.244715 -60.244715 -57.391485 -56.526644 -57.258402 -56.625158   \n",
       "307 -59.529203 -58.094118 -58.708832 -60.660041 -59.539054 -58.654146   \n",
       "308 -57.419533 -54.810391 -53.672090 -56.541297 -54.398777 -51.947087   \n",
       "\n",
       "           6          7          8          9     ...           207  \\\n",
       "298 -50.169745 -50.886473 -50.275355 -51.907351   ...    -47.518230   \n",
       "299 -47.951298 -50.378521 -49.107214 -50.084609   ...    -28.069463   \n",
       "300 -62.801707 -64.522762 -64.676879 -63.780731   ...    -58.870220   \n",
       "301 -58.498599 -58.577014 -59.843182 -59.149372   ...    -56.717537   \n",
       "302 -51.371920 -51.136957 -51.782069 -51.782069   ...    -51.499885   \n",
       "303 -69.242524 -69.242524 -66.966045 -63.217095   ...    -62.355711   \n",
       "304 -63.579745 -70.879143 -70.885193 -70.885193   ...    -59.904442   \n",
       "305 -57.617670 -60.244715 -58.908708 -57.539194   ...    -53.254220   \n",
       "307 -57.798042 -56.422977 -56.930666 -59.312142   ...    -57.876186   \n",
       "308 -54.291875 -54.611404 -55.476483 -56.366090   ...    -58.682303   \n",
       "\n",
       "           208        209        210        211        212        213  \\\n",
       "298 -46.052274 -45.574740 -46.517232 -48.663554 -49.160694 -51.180327   \n",
       "299 -27.181224 -28.187199 -28.794791 -28.706694 -30.295817 -31.448918   \n",
       "300 -61.683645 -59.392167 -58.266269 -60.922640 -64.676879 -64.676879   \n",
       "301 -58.949967 -57.629906 -59.174890 -58.905429 -57.774437 -63.483848   \n",
       "302 -51.476879 -51.782069 -51.782069 -51.782069 -51.323238 -50.613737   \n",
       "303 -61.165540 -61.309433        NaN        NaN        NaN        NaN   \n",
       "304 -65.932897 -68.419112 -66.637226 -63.676880 -59.776530 -59.531735   \n",
       "305 -52.002472 -50.719661 -50.529916 -53.913672 -54.701472 -53.199863   \n",
       "307 -54.990796 -55.307518 -61.926010 -56.977314 -54.574473 -53.689643   \n",
       "308 -60.253011 -59.709635 -58.438877 -59.303890 -63.062989 -65.427534   \n",
       "\n",
       "           214        215      0    \n",
       "298 -47.262994 -45.473499  disgust  \n",
       "299 -29.981349 -29.475319  disgust  \n",
       "300 -64.676879 -64.178914  fearful  \n",
       "301 -65.929731 -64.601276      sad  \n",
       "302 -50.840188 -50.942781  fearful  \n",
       "303        NaN        NaN     calm  \n",
       "304 -65.806624 -65.705298     calm  \n",
       "305 -51.693457 -50.263608    happy  \n",
       "307 -55.768147 -58.674321  disgust  \n",
       "308 -65.043220 -65.979849      sad  \n",
       "\n",
       "[10 rows x 217 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[250:260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-53.756179</td>\n",
       "      <td>-49.120491</td>\n",
       "      <td>-47.523206</td>\n",
       "      <td>-49.048604</td>\n",
       "      <td>-51.008211</td>\n",
       "      <td>-51.015084</td>\n",
       "      <td>-50.169745</td>\n",
       "      <td>-50.886473</td>\n",
       "      <td>-50.275355</td>\n",
       "      <td>-51.907351</td>\n",
       "      <td>...</td>\n",
       "      <td>-47.518230</td>\n",
       "      <td>-46.052274</td>\n",
       "      <td>-45.574740</td>\n",
       "      <td>-46.517232</td>\n",
       "      <td>-48.663554</td>\n",
       "      <td>-49.160694</td>\n",
       "      <td>-51.180327</td>\n",
       "      <td>-47.262994</td>\n",
       "      <td>-45.473499</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-48.351829</td>\n",
       "      <td>-47.869963</td>\n",
       "      <td>-46.828234</td>\n",
       "      <td>-49.911606</td>\n",
       "      <td>-50.994150</td>\n",
       "      <td>-47.716347</td>\n",
       "      <td>-47.951298</td>\n",
       "      <td>-50.378521</td>\n",
       "      <td>-49.107214</td>\n",
       "      <td>-50.084609</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.069463</td>\n",
       "      <td>-27.181224</td>\n",
       "      <td>-28.187199</td>\n",
       "      <td>-28.794791</td>\n",
       "      <td>-28.706694</td>\n",
       "      <td>-30.295817</td>\n",
       "      <td>-31.448918</td>\n",
       "      <td>-29.981349</td>\n",
       "      <td>-29.475319</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-61.363933</td>\n",
       "      <td>-60.424969</td>\n",
       "      <td>-60.542148</td>\n",
       "      <td>-63.737012</td>\n",
       "      <td>-64.673218</td>\n",
       "      <td>-61.401023</td>\n",
       "      <td>-62.801707</td>\n",
       "      <td>-64.522762</td>\n",
       "      <td>-64.676879</td>\n",
       "      <td>-63.780731</td>\n",
       "      <td>...</td>\n",
       "      <td>-58.870220</td>\n",
       "      <td>-61.683645</td>\n",
       "      <td>-59.392167</td>\n",
       "      <td>-58.266269</td>\n",
       "      <td>-60.922640</td>\n",
       "      <td>-64.676879</td>\n",
       "      <td>-64.676879</td>\n",
       "      <td>-64.676879</td>\n",
       "      <td>-64.178914</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>-62.691257</td>\n",
       "      <td>-61.921366</td>\n",
       "      <td>-58.112355</td>\n",
       "      <td>-55.378601</td>\n",
       "      <td>-56.545518</td>\n",
       "      <td>-58.246249</td>\n",
       "      <td>-58.498599</td>\n",
       "      <td>-58.577014</td>\n",
       "      <td>-59.843182</td>\n",
       "      <td>-59.149372</td>\n",
       "      <td>...</td>\n",
       "      <td>-56.717537</td>\n",
       "      <td>-58.949967</td>\n",
       "      <td>-57.629906</td>\n",
       "      <td>-59.174890</td>\n",
       "      <td>-58.905429</td>\n",
       "      <td>-57.774437</td>\n",
       "      <td>-63.483848</td>\n",
       "      <td>-65.929731</td>\n",
       "      <td>-64.601276</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>-47.560947</td>\n",
       "      <td>-48.092271</td>\n",
       "      <td>-48.728114</td>\n",
       "      <td>-49.372154</td>\n",
       "      <td>-50.501240</td>\n",
       "      <td>-51.668273</td>\n",
       "      <td>-51.371920</td>\n",
       "      <td>-51.136957</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>...</td>\n",
       "      <td>-51.499885</td>\n",
       "      <td>-51.476879</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>-51.782069</td>\n",
       "      <td>-51.323238</td>\n",
       "      <td>-50.613737</td>\n",
       "      <td>-50.840188</td>\n",
       "      <td>-50.942781</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>-58.867968</td>\n",
       "      <td>-58.666244</td>\n",
       "      <td>-59.207609</td>\n",
       "      <td>-61.005363</td>\n",
       "      <td>-63.547246</td>\n",
       "      <td>-66.702907</td>\n",
       "      <td>-69.242524</td>\n",
       "      <td>-69.242524</td>\n",
       "      <td>-66.966045</td>\n",
       "      <td>-63.217095</td>\n",
       "      <td>...</td>\n",
       "      <td>-62.355711</td>\n",
       "      <td>-61.165540</td>\n",
       "      <td>-61.309433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>-63.396786</td>\n",
       "      <td>-61.330426</td>\n",
       "      <td>-59.724636</td>\n",
       "      <td>-59.734818</td>\n",
       "      <td>-59.192870</td>\n",
       "      <td>-60.495081</td>\n",
       "      <td>-63.579745</td>\n",
       "      <td>-70.879143</td>\n",
       "      <td>-70.885193</td>\n",
       "      <td>-70.885193</td>\n",
       "      <td>...</td>\n",
       "      <td>-59.904442</td>\n",
       "      <td>-65.932897</td>\n",
       "      <td>-68.419112</td>\n",
       "      <td>-66.637226</td>\n",
       "      <td>-63.676880</td>\n",
       "      <td>-59.776530</td>\n",
       "      <td>-59.531735</td>\n",
       "      <td>-65.806624</td>\n",
       "      <td>-65.705298</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>-60.244715</td>\n",
       "      <td>-60.244715</td>\n",
       "      <td>-57.391485</td>\n",
       "      <td>-56.526644</td>\n",
       "      <td>-57.258402</td>\n",
       "      <td>-56.625158</td>\n",
       "      <td>-57.617670</td>\n",
       "      <td>-60.244715</td>\n",
       "      <td>-58.908708</td>\n",
       "      <td>-57.539194</td>\n",
       "      <td>...</td>\n",
       "      <td>-53.254220</td>\n",
       "      <td>-52.002472</td>\n",
       "      <td>-50.719661</td>\n",
       "      <td>-50.529916</td>\n",
       "      <td>-53.913672</td>\n",
       "      <td>-54.701472</td>\n",
       "      <td>-53.199863</td>\n",
       "      <td>-51.693457</td>\n",
       "      <td>-50.263608</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>-59.529203</td>\n",
       "      <td>-58.094118</td>\n",
       "      <td>-58.708832</td>\n",
       "      <td>-60.660041</td>\n",
       "      <td>-59.539054</td>\n",
       "      <td>-58.654146</td>\n",
       "      <td>-57.798042</td>\n",
       "      <td>-56.422977</td>\n",
       "      <td>-56.930666</td>\n",
       "      <td>-59.312142</td>\n",
       "      <td>...</td>\n",
       "      <td>-57.876186</td>\n",
       "      <td>-54.990796</td>\n",
       "      <td>-55.307518</td>\n",
       "      <td>-61.926010</td>\n",
       "      <td>-56.977314</td>\n",
       "      <td>-54.574473</td>\n",
       "      <td>-53.689643</td>\n",
       "      <td>-55.768147</td>\n",
       "      <td>-58.674321</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>-57.419533</td>\n",
       "      <td>-54.810391</td>\n",
       "      <td>-53.672090</td>\n",
       "      <td>-56.541297</td>\n",
       "      <td>-54.398777</td>\n",
       "      <td>-51.947087</td>\n",
       "      <td>-54.291875</td>\n",
       "      <td>-54.611404</td>\n",
       "      <td>-55.476483</td>\n",
       "      <td>-56.366090</td>\n",
       "      <td>...</td>\n",
       "      <td>-58.682303</td>\n",
       "      <td>-60.253011</td>\n",
       "      <td>-59.709635</td>\n",
       "      <td>-58.438877</td>\n",
       "      <td>-59.303890</td>\n",
       "      <td>-63.062989</td>\n",
       "      <td>-65.427534</td>\n",
       "      <td>-65.043220</td>\n",
       "      <td>-65.979849</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5    \\\n",
       "298 -53.756179 -49.120491 -47.523206 -49.048604 -51.008211 -51.015084   \n",
       "299 -48.351829 -47.869963 -46.828234 -49.911606 -50.994150 -47.716347   \n",
       "300 -61.363933 -60.424969 -60.542148 -63.737012 -64.673218 -61.401023   \n",
       "301 -62.691257 -61.921366 -58.112355 -55.378601 -56.545518 -58.246249   \n",
       "302 -47.560947 -48.092271 -48.728114 -49.372154 -50.501240 -51.668273   \n",
       "303 -58.867968 -58.666244 -59.207609 -61.005363 -63.547246 -66.702907   \n",
       "304 -63.396786 -61.330426 -59.724636 -59.734818 -59.192870 -60.495081   \n",
       "305 -60.244715 -60.244715 -57.391485 -56.526644 -57.258402 -56.625158   \n",
       "307 -59.529203 -58.094118 -58.708832 -60.660041 -59.539054 -58.654146   \n",
       "308 -57.419533 -54.810391 -53.672090 -56.541297 -54.398777 -51.947087   \n",
       "\n",
       "           6          7          8          9     ...           207  \\\n",
       "298 -50.169745 -50.886473 -50.275355 -51.907351   ...    -47.518230   \n",
       "299 -47.951298 -50.378521 -49.107214 -50.084609   ...    -28.069463   \n",
       "300 -62.801707 -64.522762 -64.676879 -63.780731   ...    -58.870220   \n",
       "301 -58.498599 -58.577014 -59.843182 -59.149372   ...    -56.717537   \n",
       "302 -51.371920 -51.136957 -51.782069 -51.782069   ...    -51.499885   \n",
       "303 -69.242524 -69.242524 -66.966045 -63.217095   ...    -62.355711   \n",
       "304 -63.579745 -70.879143 -70.885193 -70.885193   ...    -59.904442   \n",
       "305 -57.617670 -60.244715 -58.908708 -57.539194   ...    -53.254220   \n",
       "307 -57.798042 -56.422977 -56.930666 -59.312142   ...    -57.876186   \n",
       "308 -54.291875 -54.611404 -55.476483 -56.366090   ...    -58.682303   \n",
       "\n",
       "           208        209        210        211        212        213  \\\n",
       "298 -46.052274 -45.574740 -46.517232 -48.663554 -49.160694 -51.180327   \n",
       "299 -27.181224 -28.187199 -28.794791 -28.706694 -30.295817 -31.448918   \n",
       "300 -61.683645 -59.392167 -58.266269 -60.922640 -64.676879 -64.676879   \n",
       "301 -58.949967 -57.629906 -59.174890 -58.905429 -57.774437 -63.483848   \n",
       "302 -51.476879 -51.782069 -51.782069 -51.782069 -51.323238 -50.613737   \n",
       "303 -61.165540 -61.309433   0.000000   0.000000   0.000000   0.000000   \n",
       "304 -65.932897 -68.419112 -66.637226 -63.676880 -59.776530 -59.531735   \n",
       "305 -52.002472 -50.719661 -50.529916 -53.913672 -54.701472 -53.199863   \n",
       "307 -54.990796 -55.307518 -61.926010 -56.977314 -54.574473 -53.689643   \n",
       "308 -60.253011 -59.709635 -58.438877 -59.303890 -63.062989 -65.427534   \n",
       "\n",
       "           214        215      0    \n",
       "298 -47.262994 -45.473499  disgust  \n",
       "299 -29.981349 -29.475319  disgust  \n",
       "300 -64.676879 -64.178914  fearful  \n",
       "301 -65.929731 -64.601276      sad  \n",
       "302 -50.840188 -50.942781  fearful  \n",
       "303   0.000000   0.000000     calm  \n",
       "304 -65.806624 -65.705298     calm  \n",
       "305 -51.693457 -50.263608    happy  \n",
       "307 -55.768147 -58.674321  disgust  \n",
       "308 -65.043220 -65.979849      sad  \n",
       "\n",
       "[10 rows x 217 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[250:260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfeatures = train.iloc[:, :-1]\n",
    "trainlabel = train.iloc[:, -1:]\n",
    "testfeatures = test.iloc[:, :-1]\n",
    "testlabel = test.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train = np.array(trainfeatures)\n",
    "y_train = np.array(trainlabel)\n",
    "X_test = np.array(testfeatures)\n",
    "y_test = np.array(testlabel)\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = np_utils.to_categorical(lb.fit_transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1171, 216)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_traincnn =np.expand_dims(X_train, axis=2)\n",
    "x_testcnn= np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer conv1d_16: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-2b22f2ad7c6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.add(Conv1D(128, 5,padding='same',\n\u001b[0;32m----> 3\u001b[0;31m                  input_shape=(256,1)))\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    487\u001b[0m                           output_shapes=[self.outputs[0]._keras_shape])\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    456\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer conv1d_16: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(128, 5,padding='same',\n",
    "                 input_shape=(256,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_11 (Conv1D)           (None, 216, 256)          1536      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 216, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 216, 128)          163968    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 3, 128)            82048     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 3080      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 414,728\n",
      "Trainable params: 414,728\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1171 samples, validate on 269 samples\n",
      "Epoch 1/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 2.0735 - acc: 0.1401 - val_loss: 2.0740 - val_acc: 0.1375\n",
      "Epoch 2/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 2.0695 - acc: 0.1674 - val_loss: 2.0716 - val_acc: 0.1190\n",
      "Epoch 3/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0667 - acc: 0.1682 - val_loss: 2.0699 - val_acc: 0.1264\n",
      "Epoch 4/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0656 - acc: 0.1674 - val_loss: 2.0686 - val_acc: 0.1078\n",
      "Epoch 5/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 2.0634 - acc: 0.1691 - val_loss: 2.0675 - val_acc: 0.0967\n",
      "Epoch 6/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0623 - acc: 0.1614 - val_loss: 2.0661 - val_acc: 0.1078\n",
      "Epoch 7/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0603 - acc: 0.1623 - val_loss: 2.0650 - val_acc: 0.1078\n",
      "Epoch 8/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0591 - acc: 0.1674 - val_loss: 2.0642 - val_acc: 0.1338\n",
      "Epoch 9/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0571 - acc: 0.1751 - val_loss: 2.0631 - val_acc: 0.1115\n",
      "Epoch 10/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0567 - acc: 0.1657 - val_loss: 2.0621 - val_acc: 0.1115\n",
      "Epoch 11/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0554 - acc: 0.1725 - val_loss: 2.0613 - val_acc: 0.1115\n",
      "Epoch 12/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0544 - acc: 0.1648 - val_loss: 2.0599 - val_acc: 0.1227\n",
      "Epoch 13/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0537 - acc: 0.1759 - val_loss: 2.0592 - val_acc: 0.1524\n",
      "Epoch 14/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0519 - acc: 0.1802 - val_loss: 2.0583 - val_acc: 0.1524\n",
      "Epoch 15/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0509 - acc: 0.1845 - val_loss: 2.0583 - val_acc: 0.1599\n",
      "Epoch 16/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0498 - acc: 0.1913 - val_loss: 2.0555 - val_acc: 0.1413\n",
      "Epoch 17/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0482 - acc: 0.1862 - val_loss: 2.0543 - val_acc: 0.1413\n",
      "Epoch 18/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 2.0463 - acc: 0.1879 - val_loss: 2.0545 - val_acc: 0.1599\n",
      "Epoch 19/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0449 - acc: 0.1981 - val_loss: 2.0509 - val_acc: 0.1450\n",
      "Epoch 20/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 2.0436 - acc: 0.2058 - val_loss: 2.0496 - val_acc: 0.1375\n",
      "Epoch 21/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0405 - acc: 0.2024 - val_loss: 2.0507 - val_acc: 0.1599\n",
      "Epoch 22/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0386 - acc: 0.2024 - val_loss: 2.0468 - val_acc: 0.1413\n",
      "Epoch 23/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0379 - acc: 0.2041 - val_loss: 2.0454 - val_acc: 0.1413\n",
      "Epoch 24/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0352 - acc: 0.2101 - val_loss: 2.0446 - val_acc: 0.1599\n",
      "Epoch 25/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 2.0335 - acc: 0.2109 - val_loss: 2.0440 - val_acc: 0.1636\n",
      "Epoch 26/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 2.0318 - acc: 0.2161 - val_loss: 2.0427 - val_acc: 0.1599\n",
      "Epoch 27/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0314 - acc: 0.2135 - val_loss: 2.0388 - val_acc: 0.1413\n",
      "Epoch 28/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 2.0284 - acc: 0.2229 - val_loss: 2.0370 - val_acc: 0.1413\n",
      "Epoch 29/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 2.0255 - acc: 0.2289 - val_loss: 2.0382 - val_acc: 0.1599\n",
      "Epoch 30/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 2.0240 - acc: 0.2237 - val_loss: 2.0333 - val_acc: 0.1450\n",
      "Epoch 31/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 2.0213 - acc: 0.2237 - val_loss: 2.0312 - val_acc: 0.1450\n",
      "Epoch 32/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 2.0200 - acc: 0.2374 - val_loss: 2.0295 - val_acc: 0.1673\n",
      "Epoch 33/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0167 - acc: 0.2297 - val_loss: 2.0275 - val_acc: 0.1673\n",
      "Epoch 34/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0135 - acc: 0.2340 - val_loss: 2.0239 - val_acc: 0.1524\n",
      "Epoch 35/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0111 - acc: 0.2391 - val_loss: 2.0227 - val_acc: 0.1784\n",
      "Epoch 36/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 2.0093 - acc: 0.2442 - val_loss: 2.0203 - val_acc: 0.1784\n",
      "Epoch 37/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 2.0059 - acc: 0.2477 - val_loss: 2.0183 - val_acc: 0.1896\n",
      "Epoch 38/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 2.0031 - acc: 0.2502 - val_loss: 2.0147 - val_acc: 0.1784\n",
      "Epoch 39/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 2.0008 - acc: 0.2468 - val_loss: 2.0120 - val_acc: 0.1747\n",
      "Epoch 40/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.9976 - acc: 0.2434 - val_loss: 2.0103 - val_acc: 0.1896\n",
      "Epoch 41/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.9940 - acc: 0.2468 - val_loss: 2.0066 - val_acc: 0.1710\n",
      "Epoch 42/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.9912 - acc: 0.2570 - val_loss: 2.0036 - val_acc: 0.1859\n",
      "Epoch 43/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.9876 - acc: 0.2519 - val_loss: 2.0030 - val_acc: 0.2045\n",
      "Epoch 44/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.9837 - acc: 0.2690 - val_loss: 2.0061 - val_acc: 0.1933\n",
      "Epoch 45/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.9820 - acc: 0.2639 - val_loss: 2.0038 - val_acc: 0.1970\n",
      "Epoch 46/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.9796 - acc: 0.2613 - val_loss: 1.9929 - val_acc: 0.1970\n",
      "Epoch 47/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.9739 - acc: 0.2545 - val_loss: 2.0071 - val_acc: 0.1747\n",
      "Epoch 48/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.9740 - acc: 0.2707 - val_loss: 1.9917 - val_acc: 0.2156\n",
      "Epoch 49/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.9678 - acc: 0.2562 - val_loss: 1.9834 - val_acc: 0.2119\n",
      "Epoch 50/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.9670 - acc: 0.2622 - val_loss: 1.9983 - val_acc: 0.1896\n",
      "Epoch 51/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.9642 - acc: 0.2596 - val_loss: 1.9788 - val_acc: 0.2193\n",
      "Epoch 52/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.9611 - acc: 0.2570 - val_loss: 1.9816 - val_acc: 0.2156\n",
      "Epoch 53/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.9566 - acc: 0.2545 - val_loss: 1.9723 - val_acc: 0.2156\n",
      "Epoch 54/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.9527 - acc: 0.2536 - val_loss: 1.9722 - val_acc: 0.2119\n",
      "Epoch 55/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.9482 - acc: 0.2579 - val_loss: 1.9789 - val_acc: 0.2230\n",
      "Epoch 56/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.9495 - acc: 0.2639 - val_loss: 2.0175 - val_acc: 0.1450\n",
      "Epoch 57/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.9464 - acc: 0.2459 - val_loss: 1.9729 - val_acc: 0.2193\n",
      "Epoch 58/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.9414 - acc: 0.2605 - val_loss: 1.9586 - val_acc: 0.2379\n",
      "Epoch 59/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.9383 - acc: 0.2622 - val_loss: 1.9954 - val_acc: 0.1673\n",
      "Epoch 60/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.9349 - acc: 0.2630 - val_loss: 1.9553 - val_acc: 0.2268\n",
      "Epoch 61/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.9313 - acc: 0.2562 - val_loss: 1.9505 - val_acc: 0.2193\n",
      "Epoch 62/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.9313 - acc: 0.2699 - val_loss: 1.9532 - val_acc: 0.2305\n",
      "Epoch 63/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.9243 - acc: 0.2767 - val_loss: 2.0251 - val_acc: 0.1524\n",
      "Epoch 64/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.9243 - acc: 0.2605 - val_loss: 1.9583 - val_acc: 0.2268\n",
      "Epoch 65/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.9196 - acc: 0.2596 - val_loss: 1.9430 - val_acc: 0.2379\n",
      "Epoch 66/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.9138 - acc: 0.2775 - val_loss: 1.9440 - val_acc: 0.2379\n",
      "Epoch 67/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.9133 - acc: 0.2741 - val_loss: 1.9611 - val_acc: 0.2045\n",
      "Epoch 68/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.9116 - acc: 0.2741 - val_loss: 1.9323 - val_acc: 0.2305\n",
      "Epoch 69/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.9051 - acc: 0.2622 - val_loss: 1.9482 - val_acc: 0.2230\n",
      "Epoch 70/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.9002 - acc: 0.2690 - val_loss: 1.9713 - val_acc: 0.2007\n",
      "Epoch 71/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.8992 - acc: 0.2818 - val_loss: 1.9265 - val_acc: 0.2342\n",
      "Epoch 72/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.8975 - acc: 0.2750 - val_loss: 1.9265 - val_acc: 0.2416\n",
      "Epoch 73/700\n",
      "1171/1171 [==============================] - 15s 13ms/step - loss: 1.8915 - acc: 0.2724 - val_loss: 1.9541 - val_acc: 0.2007\n",
      "Epoch 74/700\n",
      "1171/1171 [==============================] - 12s 11ms/step - loss: 1.8943 - acc: 0.2673 - val_loss: 1.9604 - val_acc: 0.1970\n",
      "Epoch 75/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8949 - acc: 0.2605 - val_loss: 1.9380 - val_acc: 0.2342\n",
      "Epoch 76/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8898 - acc: 0.2775 - val_loss: 1.9269 - val_acc: 0.2416\n",
      "Epoch 77/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8873 - acc: 0.2750 - val_loss: 1.9452 - val_acc: 0.2007\n",
      "Epoch 78/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8858 - acc: 0.2699 - val_loss: 1.9123 - val_acc: 0.2305\n",
      "Epoch 79/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8839 - acc: 0.2622 - val_loss: 1.9724 - val_acc: 0.2007\n",
      "Epoch 80/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8772 - acc: 0.2681 - val_loss: 1.9048 - val_acc: 0.2230\n",
      "Epoch 81/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8772 - acc: 0.2784 - val_loss: 1.9022 - val_acc: 0.2193\n",
      "Epoch 82/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8734 - acc: 0.2690 - val_loss: 1.9040 - val_acc: 0.2342\n",
      "Epoch 83/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8733 - acc: 0.2758 - val_loss: 1.8991 - val_acc: 0.2305\n",
      "Epoch 84/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8691 - acc: 0.2707 - val_loss: 1.9055 - val_acc: 0.2416\n",
      "Epoch 85/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8650 - acc: 0.2733 - val_loss: 1.9050 - val_acc: 0.2416\n",
      "Epoch 86/700\n",
      "1171/1171 [==============================] - 12s 11ms/step - loss: 1.8661 - acc: 0.2818 - val_loss: 1.9098 - val_acc: 0.2454\n",
      "Epoch 87/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.8632 - acc: 0.2750 - val_loss: 1.8967 - val_acc: 0.2342\n",
      "Epoch 88/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.8609 - acc: 0.2681 - val_loss: 1.9311 - val_acc: 0.2342\n",
      "Epoch 89/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8567 - acc: 0.2784 - val_loss: 1.8934 - val_acc: 0.2379\n",
      "Epoch 90/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8550 - acc: 0.2741 - val_loss: 1.8910 - val_acc: 0.2379\n",
      "Epoch 91/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8554 - acc: 0.2810 - val_loss: 1.9125 - val_acc: 0.2416\n",
      "Epoch 92/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8502 - acc: 0.2810 - val_loss: 1.9501 - val_acc: 0.2082\n",
      "Epoch 93/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8455 - acc: 0.2886 - val_loss: 1.8809 - val_acc: 0.2342\n",
      "Epoch 94/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8484 - acc: 0.2869 - val_loss: 1.8885 - val_acc: 0.2565\n",
      "Epoch 95/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8474 - acc: 0.2724 - val_loss: 1.8968 - val_acc: 0.2416\n",
      "Epoch 96/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8436 - acc: 0.2852 - val_loss: 1.9431 - val_acc: 0.2082\n",
      "Epoch 97/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8394 - acc: 0.2878 - val_loss: 1.8764 - val_acc: 0.2342\n",
      "Epoch 98/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8381 - acc: 0.2810 - val_loss: 1.8816 - val_acc: 0.2454\n",
      "Epoch 99/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8301 - acc: 0.2792 - val_loss: 1.8930 - val_acc: 0.2416\n",
      "Epoch 100/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8337 - acc: 0.2895 - val_loss: 1.9131 - val_acc: 0.2491\n",
      "Epoch 101/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8308 - acc: 0.2835 - val_loss: 1.8794 - val_acc: 0.2454\n",
      "Epoch 102/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8294 - acc: 0.2878 - val_loss: 1.8703 - val_acc: 0.2305\n",
      "Epoch 103/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8313 - acc: 0.2724 - val_loss: 1.8653 - val_acc: 0.2602\n",
      "Epoch 104/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8308 - acc: 0.2886 - val_loss: 1.8638 - val_acc: 0.2379\n",
      "Epoch 105/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8228 - acc: 0.2912 - val_loss: 1.8863 - val_acc: 0.2454\n",
      "Epoch 106/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8227 - acc: 0.2904 - val_loss: 1.8709 - val_acc: 0.2491\n",
      "Epoch 107/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.8203 - acc: 0.2844 - val_loss: 1.8633 - val_acc: 0.2639\n",
      "Epoch 108/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.8209 - acc: 0.2904 - val_loss: 1.9164 - val_acc: 0.2528\n",
      "Epoch 109/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.8166 - acc: 0.2861 - val_loss: 1.8634 - val_acc: 0.2454\n",
      "Epoch 110/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.8169 - acc: 0.2878 - val_loss: 1.8569 - val_acc: 0.2230\n",
      "Epoch 111/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.8089 - acc: 0.3040 - val_loss: 1.8560 - val_acc: 0.2639\n",
      "Epoch 112/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.8183 - acc: 0.2886 - val_loss: 1.8551 - val_acc: 0.2342\n",
      "Epoch 113/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.8123 - acc: 0.2955 - val_loss: 1.8670 - val_acc: 0.2379\n",
      "Epoch 114/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.8104 - acc: 0.2972 - val_loss: 1.8522 - val_acc: 0.2491\n",
      "Epoch 115/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.8045 - acc: 0.2963 - val_loss: 1.8505 - val_acc: 0.2454\n",
      "Epoch 116/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.8031 - acc: 0.2861 - val_loss: 1.8715 - val_acc: 0.2454\n",
      "Epoch 117/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.8005 - acc: 0.2921 - val_loss: 1.8765 - val_acc: 0.2639\n",
      "Epoch 118/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.8069 - acc: 0.2861 - val_loss: 1.8553 - val_acc: 0.2454\n",
      "Epoch 119/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.8054 - acc: 0.2972 - val_loss: 1.8507 - val_acc: 0.2528\n",
      "Epoch 120/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.7966 - acc: 0.2963 - val_loss: 1.8452 - val_acc: 0.2342\n",
      "Epoch 121/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7961 - acc: 0.2997 - val_loss: 1.8666 - val_acc: 0.2416\n",
      "Epoch 122/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.7922 - acc: 0.2895 - val_loss: 1.9447 - val_acc: 0.2230\n",
      "Epoch 123/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.7985 - acc: 0.2955 - val_loss: 1.8508 - val_acc: 0.2454\n",
      "Epoch 124/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7961 - acc: 0.2938 - val_loss: 1.8514 - val_acc: 0.2528\n",
      "Epoch 125/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7906 - acc: 0.2972 - val_loss: 1.8480 - val_acc: 0.2416\n",
      "Epoch 126/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7869 - acc: 0.2980 - val_loss: 1.8590 - val_acc: 0.2528\n",
      "Epoch 127/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.7937 - acc: 0.2904 - val_loss: 1.8444 - val_acc: 0.2379\n",
      "Epoch 128/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7891 - acc: 0.2929 - val_loss: 1.8478 - val_acc: 0.2491\n",
      "Epoch 129/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.7870 - acc: 0.2997 - val_loss: 1.8380 - val_acc: 0.2379\n",
      "Epoch 130/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.7846 - acc: 0.2921 - val_loss: 1.8450 - val_acc: 0.2528\n",
      "Epoch 131/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.7831 - acc: 0.3040 - val_loss: 1.8366 - val_acc: 0.2379\n",
      "Epoch 132/700\n",
      "1171/1171 [==============================] - 12s 11ms/step - loss: 1.7782 - acc: 0.3049 - val_loss: 1.8337 - val_acc: 0.2639\n",
      "Epoch 133/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.7804 - acc: 0.3006 - val_loss: 1.8309 - val_acc: 0.2491\n",
      "Epoch 134/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.7808 - acc: 0.2997 - val_loss: 1.8327 - val_acc: 0.2602\n",
      "Epoch 135/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.7799 - acc: 0.3117 - val_loss: 1.8423 - val_acc: 0.2454\n",
      "Epoch 136/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.7722 - acc: 0.3074 - val_loss: 1.8467 - val_acc: 0.2639\n",
      "Epoch 137/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.7761 - acc: 0.3100 - val_loss: 1.8382 - val_acc: 0.2416\n",
      "Epoch 138/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.7720 - acc: 0.3015 - val_loss: 1.8456 - val_acc: 0.2602\n",
      "Epoch 139/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7769 - acc: 0.3117 - val_loss: 1.8290 - val_acc: 0.2379\n",
      "Epoch 140/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7706 - acc: 0.3057 - val_loss: 1.8392 - val_acc: 0.2565\n",
      "Epoch 141/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7726 - acc: 0.3143 - val_loss: 1.8284 - val_acc: 0.2639\n",
      "Epoch 142/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7644 - acc: 0.3108 - val_loss: 1.8265 - val_acc: 0.2677\n",
      "Epoch 143/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7703 - acc: 0.3006 - val_loss: 1.8217 - val_acc: 0.2565\n",
      "Epoch 144/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7710 - acc: 0.3108 - val_loss: 1.8206 - val_acc: 0.2528\n",
      "Epoch 145/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7673 - acc: 0.3074 - val_loss: 1.8258 - val_acc: 0.2677\n",
      "Epoch 146/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7621 - acc: 0.3134 - val_loss: 1.8921 - val_acc: 0.2379\n",
      "Epoch 147/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7631 - acc: 0.3057 - val_loss: 1.8611 - val_acc: 0.2602\n",
      "Epoch 148/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7665 - acc: 0.3091 - val_loss: 1.8183 - val_acc: 0.2677\n",
      "Epoch 149/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7625 - acc: 0.3023 - val_loss: 1.8455 - val_acc: 0.2602\n",
      "Epoch 150/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7602 - acc: 0.2980 - val_loss: 1.8154 - val_acc: 0.2714\n",
      "Epoch 151/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7579 - acc: 0.3160 - val_loss: 1.8727 - val_acc: 0.2416\n",
      "Epoch 152/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7532 - acc: 0.3117 - val_loss: 1.8254 - val_acc: 0.2416\n",
      "Epoch 153/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7565 - acc: 0.3057 - val_loss: 1.8482 - val_acc: 0.2602\n",
      "Epoch 154/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7532 - acc: 0.3083 - val_loss: 1.8573 - val_acc: 0.2639\n",
      "Epoch 155/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7552 - acc: 0.3117 - val_loss: 1.8374 - val_acc: 0.2565\n",
      "Epoch 156/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7524 - acc: 0.3108 - val_loss: 1.8155 - val_acc: 0.2491\n",
      "Epoch 157/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7538 - acc: 0.3066 - val_loss: 1.8109 - val_acc: 0.2454\n",
      "Epoch 158/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7551 - acc: 0.3126 - val_loss: 1.8138 - val_acc: 0.2491\n",
      "Epoch 159/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7453 - acc: 0.3100 - val_loss: 1.8120 - val_acc: 0.2454\n",
      "Epoch 160/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7501 - acc: 0.3134 - val_loss: 1.8100 - val_acc: 0.2565\n",
      "Epoch 161/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7471 - acc: 0.2997 - val_loss: 1.8213 - val_acc: 0.2454\n",
      "Epoch 162/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7454 - acc: 0.3134 - val_loss: 1.8098 - val_acc: 0.2528\n",
      "Epoch 163/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7449 - acc: 0.3160 - val_loss: 1.8308 - val_acc: 0.2677\n",
      "Epoch 164/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7451 - acc: 0.3100 - val_loss: 1.8135 - val_acc: 0.2416\n",
      "Epoch 165/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7439 - acc: 0.3160 - val_loss: 1.8154 - val_acc: 0.2454\n",
      "Epoch 166/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7415 - acc: 0.3117 - val_loss: 1.8053 - val_acc: 0.2602\n",
      "Epoch 167/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7423 - acc: 0.3049 - val_loss: 1.8059 - val_acc: 0.2788\n",
      "Epoch 168/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7341 - acc: 0.3237 - val_loss: 1.8071 - val_acc: 0.2528\n",
      "Epoch 169/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7395 - acc: 0.3151 - val_loss: 1.8372 - val_acc: 0.2677\n",
      "Epoch 170/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7405 - acc: 0.3091 - val_loss: 1.8557 - val_acc: 0.2528\n",
      "Epoch 171/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7365 - acc: 0.3177 - val_loss: 1.8277 - val_acc: 0.2714\n",
      "Epoch 172/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7334 - acc: 0.3160 - val_loss: 1.8508 - val_acc: 0.2565\n",
      "Epoch 173/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7310 - acc: 0.3271 - val_loss: 1.7985 - val_acc: 0.2788\n",
      "Epoch 174/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7322 - acc: 0.3254 - val_loss: 1.8214 - val_acc: 0.2751\n",
      "Epoch 175/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7353 - acc: 0.3202 - val_loss: 1.7988 - val_acc: 0.2788\n",
      "Epoch 176/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7297 - acc: 0.3134 - val_loss: 1.8453 - val_acc: 0.2602\n",
      "Epoch 177/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7272 - acc: 0.3322 - val_loss: 1.8037 - val_acc: 0.2454\n",
      "Epoch 178/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7263 - acc: 0.3219 - val_loss: 1.8685 - val_acc: 0.2379\n",
      "Epoch 179/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7230 - acc: 0.3279 - val_loss: 1.7962 - val_acc: 0.2788\n",
      "Epoch 180/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7252 - acc: 0.3168 - val_loss: 1.8177 - val_acc: 0.2751\n",
      "Epoch 181/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7224 - acc: 0.3339 - val_loss: 1.8037 - val_acc: 0.2416\n",
      "Epoch 182/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7189 - acc: 0.3237 - val_loss: 1.8171 - val_acc: 0.2677\n",
      "Epoch 183/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7164 - acc: 0.3202 - val_loss: 1.7958 - val_acc: 0.2639\n",
      "Epoch 184/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.7207 - acc: 0.3279 - val_loss: 1.8152 - val_acc: 0.2714\n",
      "Epoch 185/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7179 - acc: 0.3322 - val_loss: 1.8761 - val_acc: 0.2416\n",
      "Epoch 186/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7135 - acc: 0.3313 - val_loss: 1.7932 - val_acc: 0.2714\n",
      "Epoch 187/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7169 - acc: 0.3356 - val_loss: 1.8764 - val_acc: 0.2342\n",
      "Epoch 188/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7187 - acc: 0.3177 - val_loss: 1.9227 - val_acc: 0.2305\n",
      "Epoch 189/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7174 - acc: 0.3271 - val_loss: 1.7920 - val_acc: 0.2900\n",
      "Epoch 190/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.7126 - acc: 0.3262 - val_loss: 1.7896 - val_acc: 0.2751\n",
      "Epoch 191/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7115 - acc: 0.3271 - val_loss: 1.8201 - val_acc: 0.2639\n",
      "Epoch 192/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7141 - acc: 0.3339 - val_loss: 1.8073 - val_acc: 0.2677\n",
      "Epoch 193/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7171 - acc: 0.3211 - val_loss: 1.8037 - val_acc: 0.2602\n",
      "Epoch 194/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7163 - acc: 0.3288 - val_loss: 1.7976 - val_acc: 0.2491\n",
      "Epoch 195/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7124 - acc: 0.3296 - val_loss: 1.7890 - val_acc: 0.2788\n",
      "Epoch 196/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7152 - acc: 0.3245 - val_loss: 1.8151 - val_acc: 0.2714\n",
      "Epoch 197/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7132 - acc: 0.3279 - val_loss: 1.8124 - val_acc: 0.2677\n",
      "Epoch 198/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7136 - acc: 0.3330 - val_loss: 1.8333 - val_acc: 0.2639\n",
      "Epoch 199/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7063 - acc: 0.3467 - val_loss: 1.7867 - val_acc: 0.2862\n",
      "Epoch 200/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7092 - acc: 0.3245 - val_loss: 1.8707 - val_acc: 0.2379\n",
      "Epoch 201/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.7079 - acc: 0.3365 - val_loss: 1.9366 - val_acc: 0.2454\n",
      "Epoch 202/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.7030 - acc: 0.3399 - val_loss: 1.8116 - val_acc: 0.2602\n",
      "Epoch 203/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7026 - acc: 0.3296 - val_loss: 1.7865 - val_acc: 0.2862\n",
      "Epoch 204/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7056 - acc: 0.3305 - val_loss: 1.7939 - val_acc: 0.2937\n",
      "Epoch 205/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.7037 - acc: 0.3296 - val_loss: 1.7936 - val_acc: 0.3011\n",
      "Epoch 206/700\n",
      "1171/1171 [==============================] - 15s 13ms/step - loss: 1.7024 - acc: 0.3390 - val_loss: 1.9300 - val_acc: 0.2342\n",
      "Epoch 207/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.7055 - acc: 0.3279 - val_loss: 1.7882 - val_acc: 0.2528\n",
      "Epoch 208/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6981 - acc: 0.3424 - val_loss: 1.7834 - val_acc: 0.2528\n",
      "Epoch 209/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6945 - acc: 0.3356 - val_loss: 1.8303 - val_acc: 0.2677\n",
      "Epoch 210/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6950 - acc: 0.3467 - val_loss: 1.8661 - val_acc: 0.2491\n",
      "Epoch 211/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6985 - acc: 0.3288 - val_loss: 1.7842 - val_acc: 0.2900\n",
      "Epoch 212/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6990 - acc: 0.3373 - val_loss: 1.7819 - val_acc: 0.2751\n",
      "Epoch 213/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6933 - acc: 0.3330 - val_loss: 1.8615 - val_acc: 0.2454\n",
      "Epoch 214/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.7007 - acc: 0.3228 - val_loss: 1.7790 - val_acc: 0.2751\n",
      "Epoch 215/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6968 - acc: 0.3305 - val_loss: 1.8544 - val_acc: 0.2416\n",
      "Epoch 216/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6898 - acc: 0.3450 - val_loss: 1.7960 - val_acc: 0.2714\n",
      "Epoch 217/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6900 - acc: 0.3629 - val_loss: 1.8054 - val_acc: 0.2751\n",
      "Epoch 218/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6852 - acc: 0.3356 - val_loss: 1.7912 - val_acc: 0.2639\n",
      "Epoch 219/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6910 - acc: 0.3305 - val_loss: 1.7804 - val_acc: 0.2825\n",
      "Epoch 220/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6937 - acc: 0.3407 - val_loss: 1.7894 - val_acc: 0.2677\n",
      "Epoch 221/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6882 - acc: 0.3424 - val_loss: 1.9008 - val_acc: 0.2305\n",
      "Epoch 222/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6903 - acc: 0.3313 - val_loss: 1.8116 - val_acc: 0.2714\n",
      "Epoch 223/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6844 - acc: 0.3424 - val_loss: 1.8214 - val_acc: 0.2639\n",
      "Epoch 224/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6813 - acc: 0.3390 - val_loss: 1.7837 - val_acc: 0.2602\n",
      "Epoch 225/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6886 - acc: 0.3433 - val_loss: 1.8352 - val_acc: 0.2677\n",
      "Epoch 226/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6804 - acc: 0.3442 - val_loss: 1.7865 - val_acc: 0.2602\n",
      "Epoch 227/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6846 - acc: 0.3305 - val_loss: 1.7754 - val_acc: 0.2751\n",
      "Epoch 228/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6745 - acc: 0.3518 - val_loss: 1.8204 - val_acc: 0.2677\n",
      "Epoch 229/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6799 - acc: 0.3356 - val_loss: 1.7754 - val_acc: 0.2862\n",
      "Epoch 230/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6813 - acc: 0.3442 - val_loss: 1.8321 - val_acc: 0.2602\n",
      "Epoch 231/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6734 - acc: 0.3459 - val_loss: 1.7799 - val_acc: 0.2602\n",
      "Epoch 232/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6785 - acc: 0.3493 - val_loss: 1.7871 - val_acc: 0.2714\n",
      "Epoch 233/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.6755 - acc: 0.3459 - val_loss: 1.8383 - val_acc: 0.2639\n",
      "Epoch 234/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6788 - acc: 0.3561 - val_loss: 1.9401 - val_acc: 0.2379\n",
      "Epoch 235/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6806 - acc: 0.3365 - val_loss: 1.7850 - val_acc: 0.2639\n",
      "Epoch 236/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6783 - acc: 0.3501 - val_loss: 1.9018 - val_acc: 0.2268\n",
      "Epoch 237/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6764 - acc: 0.3330 - val_loss: 1.9080 - val_acc: 0.2305\n",
      "Epoch 238/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6711 - acc: 0.3467 - val_loss: 1.8845 - val_acc: 0.2305\n",
      "Epoch 239/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6773 - acc: 0.3424 - val_loss: 1.8576 - val_acc: 0.2416\n",
      "Epoch 240/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6674 - acc: 0.3587 - val_loss: 1.8001 - val_acc: 0.2862\n",
      "Epoch 241/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6704 - acc: 0.3510 - val_loss: 1.7936 - val_acc: 0.2862\n",
      "Epoch 242/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6711 - acc: 0.3484 - val_loss: 1.7756 - val_acc: 0.2714\n",
      "Epoch 243/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6665 - acc: 0.3638 - val_loss: 1.7736 - val_acc: 0.2825\n",
      "Epoch 244/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6636 - acc: 0.3561 - val_loss: 1.8518 - val_acc: 0.2602\n",
      "Epoch 245/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6613 - acc: 0.3510 - val_loss: 1.7716 - val_acc: 0.2862\n",
      "Epoch 246/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6685 - acc: 0.3399 - val_loss: 1.7686 - val_acc: 0.2862\n",
      "Epoch 247/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6684 - acc: 0.3424 - val_loss: 1.7710 - val_acc: 0.2788\n",
      "Epoch 248/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6628 - acc: 0.3672 - val_loss: 1.8234 - val_acc: 0.2751\n",
      "Epoch 249/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6640 - acc: 0.3424 - val_loss: 1.7684 - val_acc: 0.2788\n",
      "Epoch 250/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6635 - acc: 0.3553 - val_loss: 1.7806 - val_acc: 0.2825\n",
      "Epoch 251/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6622 - acc: 0.3442 - val_loss: 1.8033 - val_acc: 0.2862\n",
      "Epoch 252/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6624 - acc: 0.3527 - val_loss: 1.8569 - val_acc: 0.2491\n",
      "Epoch 253/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6611 - acc: 0.3587 - val_loss: 1.7672 - val_acc: 0.2900\n",
      "Epoch 254/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6605 - acc: 0.3612 - val_loss: 1.7672 - val_acc: 0.3011\n",
      "Epoch 255/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6614 - acc: 0.3553 - val_loss: 1.8212 - val_acc: 0.2788\n",
      "Epoch 256/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6637 - acc: 0.3493 - val_loss: 1.7962 - val_acc: 0.2862\n",
      "Epoch 257/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6577 - acc: 0.3553 - val_loss: 1.7660 - val_acc: 0.2788\n",
      "Epoch 258/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6495 - acc: 0.3553 - val_loss: 1.7895 - val_acc: 0.2900\n",
      "Epoch 259/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6526 - acc: 0.3689 - val_loss: 1.8009 - val_acc: 0.2825\n",
      "Epoch 260/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6517 - acc: 0.3604 - val_loss: 1.8820 - val_acc: 0.2491\n",
      "Epoch 261/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6558 - acc: 0.3518 - val_loss: 1.8873 - val_acc: 0.2416\n",
      "Epoch 262/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6570 - acc: 0.3587 - val_loss: 1.7663 - val_acc: 0.2900\n",
      "Epoch 263/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6443 - acc: 0.3535 - val_loss: 1.8283 - val_acc: 0.2751\n",
      "Epoch 264/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6483 - acc: 0.3553 - val_loss: 1.7650 - val_acc: 0.2937\n",
      "Epoch 265/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6567 - acc: 0.3629 - val_loss: 1.8172 - val_acc: 0.2714\n",
      "Epoch 266/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6552 - acc: 0.3553 - val_loss: 1.8045 - val_acc: 0.2862\n",
      "Epoch 267/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6508 - acc: 0.3595 - val_loss: 1.7639 - val_acc: 0.2825\n",
      "Epoch 268/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.6490 - acc: 0.3510 - val_loss: 1.7702 - val_acc: 0.2639\n",
      "Epoch 269/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6482 - acc: 0.3561 - val_loss: 1.8013 - val_acc: 0.2900\n",
      "Epoch 270/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.6467 - acc: 0.3553 - val_loss: 1.7696 - val_acc: 0.2602\n",
      "Epoch 271/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.6446 - acc: 0.3570 - val_loss: 1.7759 - val_acc: 0.2788\n",
      "Epoch 272/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.6478 - acc: 0.3570 - val_loss: 1.7796 - val_acc: 0.2937\n",
      "Epoch 273/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.6442 - acc: 0.3629 - val_loss: 1.8145 - val_acc: 0.2825\n",
      "Epoch 274/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6384 - acc: 0.3638 - val_loss: 1.7618 - val_acc: 0.2974\n",
      "Epoch 275/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6397 - acc: 0.3587 - val_loss: 1.8261 - val_acc: 0.2751\n",
      "Epoch 276/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6399 - acc: 0.3646 - val_loss: 1.7907 - val_acc: 0.2751\n",
      "Epoch 277/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.6402 - acc: 0.3553 - val_loss: 1.7622 - val_acc: 0.3011\n",
      "Epoch 278/700\n",
      "1171/1171 [==============================] - 12s 11ms/step - loss: 1.6391 - acc: 0.3535 - val_loss: 1.9308 - val_acc: 0.2454\n",
      "Epoch 279/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6442 - acc: 0.3510 - val_loss: 1.7868 - val_acc: 0.2751\n",
      "Epoch 280/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6370 - acc: 0.3715 - val_loss: 1.8000 - val_acc: 0.2900\n",
      "Epoch 281/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6355 - acc: 0.3638 - val_loss: 1.7590 - val_acc: 0.2900\n",
      "Epoch 282/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.6428 - acc: 0.3646 - val_loss: 1.7663 - val_acc: 0.2751\n",
      "Epoch 283/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6368 - acc: 0.3715 - val_loss: 1.7609 - val_acc: 0.3011\n",
      "Epoch 284/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.6326 - acc: 0.3646 - val_loss: 1.8176 - val_acc: 0.2751\n",
      "Epoch 285/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6376 - acc: 0.3664 - val_loss: 1.7914 - val_acc: 0.2788\n",
      "Epoch 286/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6336 - acc: 0.3672 - val_loss: 1.8067 - val_acc: 0.2788\n",
      "Epoch 287/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6351 - acc: 0.3629 - val_loss: 1.7563 - val_acc: 0.2974\n",
      "Epoch 288/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6313 - acc: 0.3766 - val_loss: 1.8303 - val_acc: 0.2677\n",
      "Epoch 289/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6319 - acc: 0.3681 - val_loss: 1.7564 - val_acc: 0.3048\n",
      "Epoch 290/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6273 - acc: 0.3646 - val_loss: 1.7860 - val_acc: 0.2974\n",
      "Epoch 291/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6337 - acc: 0.3638 - val_loss: 1.7680 - val_acc: 0.2900\n",
      "Epoch 292/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6356 - acc: 0.3604 - val_loss: 1.7735 - val_acc: 0.2937\n",
      "Epoch 293/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6280 - acc: 0.3706 - val_loss: 1.7568 - val_acc: 0.2974\n",
      "Epoch 294/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6270 - acc: 0.3706 - val_loss: 1.8556 - val_acc: 0.2602\n",
      "Epoch 295/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6409 - acc: 0.3587 - val_loss: 1.7855 - val_acc: 0.2900\n",
      "Epoch 296/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6302 - acc: 0.3646 - val_loss: 1.7794 - val_acc: 0.2937\n",
      "Epoch 297/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6258 - acc: 0.3809 - val_loss: 1.7574 - val_acc: 0.2974\n",
      "Epoch 298/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6285 - acc: 0.3570 - val_loss: 1.8762 - val_acc: 0.2379\n",
      "Epoch 299/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6270 - acc: 0.3757 - val_loss: 1.7667 - val_acc: 0.2862\n",
      "Epoch 300/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6287 - acc: 0.3843 - val_loss: 1.7555 - val_acc: 0.2974\n",
      "Epoch 301/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6309 - acc: 0.3689 - val_loss: 1.8265 - val_acc: 0.2788\n",
      "Epoch 302/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6214 - acc: 0.3723 - val_loss: 1.7578 - val_acc: 0.2900\n",
      "Epoch 303/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.6288 - acc: 0.3698 - val_loss: 2.0031 - val_acc: 0.2416\n",
      "Epoch 304/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6290 - acc: 0.3757 - val_loss: 2.0069 - val_acc: 0.2416\n",
      "Epoch 305/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.6232 - acc: 0.3749 - val_loss: 1.7840 - val_acc: 0.3048\n",
      "Epoch 306/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.6149 - acc: 0.3775 - val_loss: 1.7588 - val_acc: 0.2974\n",
      "Epoch 307/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6123 - acc: 0.3740 - val_loss: 1.7800 - val_acc: 0.2974\n",
      "Epoch 308/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6153 - acc: 0.3945 - val_loss: 1.8086 - val_acc: 0.2900\n",
      "Epoch 309/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6103 - acc: 0.3800 - val_loss: 1.7553 - val_acc: 0.3011\n",
      "Epoch 310/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6196 - acc: 0.3749 - val_loss: 1.7613 - val_acc: 0.3011\n",
      "Epoch 311/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.6173 - acc: 0.3681 - val_loss: 1.8580 - val_acc: 0.2677\n",
      "Epoch 312/700\n",
      "1171/1171 [==============================] - 12s 11ms/step - loss: 1.6086 - acc: 0.3706 - val_loss: 1.7722 - val_acc: 0.2900\n",
      "Epoch 313/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6232 - acc: 0.3749 - val_loss: 1.8365 - val_acc: 0.2639\n",
      "Epoch 314/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.6093 - acc: 0.3749 - val_loss: 1.9424 - val_acc: 0.2565\n",
      "Epoch 315/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6181 - acc: 0.3749 - val_loss: 1.8765 - val_acc: 0.2491\n",
      "Epoch 316/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6203 - acc: 0.3698 - val_loss: 1.7535 - val_acc: 0.3086\n",
      "Epoch 317/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6092 - acc: 0.3715 - val_loss: 1.7867 - val_acc: 0.2937\n",
      "Epoch 318/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6090 - acc: 0.3928 - val_loss: 1.8700 - val_acc: 0.2677\n",
      "Epoch 319/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.6100 - acc: 0.3646 - val_loss: 1.7580 - val_acc: 0.3086\n",
      "Epoch 320/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.6091 - acc: 0.3860 - val_loss: 1.8496 - val_acc: 0.2639\n",
      "Epoch 321/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.6003 - acc: 0.3851 - val_loss: 1.7811 - val_acc: 0.2937\n",
      "Epoch 322/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.6110 - acc: 0.3877 - val_loss: 1.8631 - val_acc: 0.2602\n",
      "Epoch 323/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6095 - acc: 0.3826 - val_loss: 1.7607 - val_acc: 0.3123\n",
      "Epoch 324/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6085 - acc: 0.3783 - val_loss: 1.8833 - val_acc: 0.2528\n",
      "Epoch 325/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6042 - acc: 0.3766 - val_loss: 1.7728 - val_acc: 0.2974\n",
      "Epoch 326/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6084 - acc: 0.3860 - val_loss: 1.7716 - val_acc: 0.2825\n",
      "Epoch 327/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6047 - acc: 0.3954 - val_loss: 1.7641 - val_acc: 0.2937\n",
      "Epoch 328/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6053 - acc: 0.3886 - val_loss: 1.7836 - val_acc: 0.3086\n",
      "Epoch 329/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6017 - acc: 0.3834 - val_loss: 1.7528 - val_acc: 0.2974\n",
      "Epoch 330/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6111 - acc: 0.3723 - val_loss: 1.8028 - val_acc: 0.2974\n",
      "Epoch 331/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6039 - acc: 0.3843 - val_loss: 1.7817 - val_acc: 0.3086\n",
      "Epoch 332/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6026 - acc: 0.3911 - val_loss: 1.8362 - val_acc: 0.2677\n",
      "Epoch 333/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5988 - acc: 0.3954 - val_loss: 1.7501 - val_acc: 0.3086\n",
      "Epoch 334/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.6004 - acc: 0.3809 - val_loss: 1.7447 - val_acc: 0.3011\n",
      "Epoch 335/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.6021 - acc: 0.3868 - val_loss: 1.9567 - val_acc: 0.2454\n",
      "Epoch 336/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5978 - acc: 0.3894 - val_loss: 1.8787 - val_acc: 0.2602\n",
      "Epoch 337/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5999 - acc: 0.3817 - val_loss: 1.7442 - val_acc: 0.2937\n",
      "Epoch 338/700\n",
      "1171/1171 [==============================] - 12s 11ms/step - loss: 1.5964 - acc: 0.3971 - val_loss: 1.7586 - val_acc: 0.3086\n",
      "Epoch 339/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5965 - acc: 0.3843 - val_loss: 1.7493 - val_acc: 0.2974\n",
      "Epoch 340/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.6019 - acc: 0.3817 - val_loss: 1.8715 - val_acc: 0.2677\n",
      "Epoch 341/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5916 - acc: 0.3937 - val_loss: 1.7529 - val_acc: 0.3048\n",
      "Epoch 342/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5859 - acc: 0.4014 - val_loss: 1.7546 - val_acc: 0.3048\n",
      "Epoch 343/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5947 - acc: 0.3954 - val_loss: 1.9130 - val_acc: 0.2565\n",
      "Epoch 344/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5996 - acc: 0.3997 - val_loss: 1.7477 - val_acc: 0.3234\n",
      "Epoch 345/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5957 - acc: 0.3945 - val_loss: 1.9055 - val_acc: 0.2639\n",
      "Epoch 346/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5897 - acc: 0.3886 - val_loss: 1.8754 - val_acc: 0.2714\n",
      "Epoch 347/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5989 - acc: 0.3860 - val_loss: 1.7601 - val_acc: 0.2900\n",
      "Epoch 348/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5915 - acc: 0.3903 - val_loss: 1.7679 - val_acc: 0.3011\n",
      "Epoch 349/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5854 - acc: 0.3877 - val_loss: 1.7798 - val_acc: 0.2937\n",
      "Epoch 350/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5887 - acc: 0.3903 - val_loss: 1.7444 - val_acc: 0.3011\n",
      "Epoch 351/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5839 - acc: 0.4048 - val_loss: 1.7492 - val_acc: 0.2974\n",
      "Epoch 352/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5788 - acc: 0.3980 - val_loss: 1.7828 - val_acc: 0.2974\n",
      "Epoch 353/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5775 - acc: 0.4082 - val_loss: 1.9897 - val_acc: 0.2416\n",
      "Epoch 354/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5879 - acc: 0.3920 - val_loss: 1.7587 - val_acc: 0.2974\n",
      "Epoch 355/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5829 - acc: 0.4005 - val_loss: 1.8067 - val_acc: 0.2751\n",
      "Epoch 356/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5757 - acc: 0.3997 - val_loss: 1.7918 - val_acc: 0.2751\n",
      "Epoch 357/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5837 - acc: 0.3800 - val_loss: 1.7442 - val_acc: 0.3197\n",
      "Epoch 358/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5831 - acc: 0.4005 - val_loss: 1.8214 - val_acc: 0.2825\n",
      "Epoch 359/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5794 - acc: 0.4022 - val_loss: 1.8410 - val_acc: 0.2639\n",
      "Epoch 360/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5798 - acc: 0.4014 - val_loss: 1.7771 - val_acc: 0.3011\n",
      "Epoch 361/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5757 - acc: 0.4056 - val_loss: 1.7519 - val_acc: 0.3048\n",
      "Epoch 362/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5782 - acc: 0.3894 - val_loss: 1.7742 - val_acc: 0.2974\n",
      "Epoch 363/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5776 - acc: 0.3920 - val_loss: 1.9432 - val_acc: 0.2565\n",
      "Epoch 364/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5782 - acc: 0.3945 - val_loss: 1.8368 - val_acc: 0.2714\n",
      "Epoch 365/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5776 - acc: 0.3980 - val_loss: 1.7539 - val_acc: 0.3123\n",
      "Epoch 366/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5820 - acc: 0.3903 - val_loss: 1.7549 - val_acc: 0.2900\n",
      "Epoch 367/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5739 - acc: 0.3928 - val_loss: 1.7554 - val_acc: 0.3011\n",
      "Epoch 368/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5710 - acc: 0.4005 - val_loss: 1.7418 - val_acc: 0.3048\n",
      "Epoch 369/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5735 - acc: 0.4005 - val_loss: 1.7804 - val_acc: 0.2974\n",
      "Epoch 370/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5797 - acc: 0.4167 - val_loss: 1.7676 - val_acc: 0.2937\n",
      "Epoch 371/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5682 - acc: 0.4082 - val_loss: 1.7553 - val_acc: 0.3011\n",
      "Epoch 372/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5624 - acc: 0.4022 - val_loss: 1.7440 - val_acc: 0.3086\n",
      "Epoch 373/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5746 - acc: 0.3980 - val_loss: 1.8946 - val_acc: 0.2639\n",
      "Epoch 374/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5700 - acc: 0.4014 - val_loss: 1.7701 - val_acc: 0.3011\n",
      "Epoch 375/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5718 - acc: 0.3971 - val_loss: 1.7500 - val_acc: 0.3123\n",
      "Epoch 376/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5706 - acc: 0.4082 - val_loss: 1.7788 - val_acc: 0.3048\n",
      "Epoch 377/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5734 - acc: 0.4014 - val_loss: 1.8377 - val_acc: 0.2751\n",
      "Epoch 378/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5679 - acc: 0.4048 - val_loss: 1.7401 - val_acc: 0.3160\n",
      "Epoch 379/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5646 - acc: 0.4082 - val_loss: 1.8203 - val_acc: 0.2751\n",
      "Epoch 380/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5645 - acc: 0.3971 - val_loss: 1.7621 - val_acc: 0.2974\n",
      "Epoch 381/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5642 - acc: 0.4116 - val_loss: 1.8275 - val_acc: 0.2714\n",
      "Epoch 382/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5713 - acc: 0.3937 - val_loss: 1.7464 - val_acc: 0.3086\n",
      "Epoch 383/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5660 - acc: 0.3971 - val_loss: 1.7353 - val_acc: 0.3309\n",
      "Epoch 384/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5588 - acc: 0.4048 - val_loss: 1.7355 - val_acc: 0.3123\n",
      "Epoch 385/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5621 - acc: 0.4184 - val_loss: 1.7381 - val_acc: 0.3123\n",
      "Epoch 386/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5647 - acc: 0.3860 - val_loss: 1.8513 - val_acc: 0.2751\n",
      "Epoch 387/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5617 - acc: 0.4091 - val_loss: 1.7392 - val_acc: 0.3011\n",
      "Epoch 388/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5611 - acc: 0.4039 - val_loss: 1.7425 - val_acc: 0.3234\n",
      "Epoch 389/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5550 - acc: 0.4150 - val_loss: 1.7515 - val_acc: 0.3160\n",
      "Epoch 390/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5592 - acc: 0.4014 - val_loss: 1.7409 - val_acc: 0.3048\n",
      "Epoch 391/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5480 - acc: 0.4142 - val_loss: 1.7574 - val_acc: 0.3048\n",
      "Epoch 392/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5527 - acc: 0.4142 - val_loss: 1.7412 - val_acc: 0.3086\n",
      "Epoch 393/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5617 - acc: 0.4150 - val_loss: 1.8189 - val_acc: 0.2862\n",
      "Epoch 394/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5547 - acc: 0.4244 - val_loss: 1.8050 - val_acc: 0.2825\n",
      "Epoch 395/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.5629 - acc: 0.4116 - val_loss: 1.7442 - val_acc: 0.3086\n",
      "Epoch 396/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5536 - acc: 0.4244 - val_loss: 1.7328 - val_acc: 0.3271\n",
      "Epoch 397/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5535 - acc: 0.4159 - val_loss: 1.7329 - val_acc: 0.3086\n",
      "Epoch 398/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5595 - acc: 0.4099 - val_loss: 1.7473 - val_acc: 0.3011\n",
      "Epoch 399/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5539 - acc: 0.3997 - val_loss: 1.7984 - val_acc: 0.2825\n",
      "Epoch 400/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5426 - acc: 0.4073 - val_loss: 1.7328 - val_acc: 0.3160\n",
      "Epoch 401/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5541 - acc: 0.4133 - val_loss: 1.7733 - val_acc: 0.2937\n",
      "Epoch 402/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5422 - acc: 0.4202 - val_loss: 1.7330 - val_acc: 0.3086\n",
      "Epoch 403/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5465 - acc: 0.4176 - val_loss: 1.8316 - val_acc: 0.2825\n",
      "Epoch 404/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5527 - acc: 0.4091 - val_loss: 1.7848 - val_acc: 0.2825\n",
      "Epoch 405/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5407 - acc: 0.4184 - val_loss: 1.8311 - val_acc: 0.2825\n",
      "Epoch 406/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5513 - acc: 0.4176 - val_loss: 1.7534 - val_acc: 0.3123\n",
      "Epoch 407/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5454 - acc: 0.4073 - val_loss: 1.7342 - val_acc: 0.3123\n",
      "Epoch 408/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5457 - acc: 0.4253 - val_loss: 1.7435 - val_acc: 0.3160\n",
      "Epoch 409/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5388 - acc: 0.4167 - val_loss: 1.7705 - val_acc: 0.2937\n",
      "Epoch 410/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5430 - acc: 0.4278 - val_loss: 1.7793 - val_acc: 0.3160\n",
      "Epoch 411/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5468 - acc: 0.4159 - val_loss: 1.7636 - val_acc: 0.3011\n",
      "Epoch 412/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5420 - acc: 0.4227 - val_loss: 1.7358 - val_acc: 0.3123\n",
      "Epoch 413/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5391 - acc: 0.4133 - val_loss: 1.8248 - val_acc: 0.2788\n",
      "Epoch 414/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5428 - acc: 0.4150 - val_loss: 1.8173 - val_acc: 0.2974\n",
      "Epoch 415/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5381 - acc: 0.4227 - val_loss: 1.7748 - val_acc: 0.2862\n",
      "Epoch 416/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5449 - acc: 0.4184 - val_loss: 1.7528 - val_acc: 0.2862\n",
      "Epoch 417/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5363 - acc: 0.4295 - val_loss: 1.7713 - val_acc: 0.2974\n",
      "Epoch 418/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5425 - acc: 0.4219 - val_loss: 1.7962 - val_acc: 0.2937\n",
      "Epoch 419/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5407 - acc: 0.4082 - val_loss: 1.7347 - val_acc: 0.3160\n",
      "Epoch 420/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5394 - acc: 0.4176 - val_loss: 1.7318 - val_acc: 0.3309\n",
      "Epoch 421/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5435 - acc: 0.4236 - val_loss: 1.7366 - val_acc: 0.3048\n",
      "Epoch 422/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5252 - acc: 0.4347 - val_loss: 1.7497 - val_acc: 0.3048\n",
      "Epoch 423/700\n",
      "1171/1171 [==============================] - 12s 11ms/step - loss: 1.5352 - acc: 0.4176 - val_loss: 1.8708 - val_acc: 0.2751\n",
      "Epoch 424/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5303 - acc: 0.4321 - val_loss: 1.7413 - val_acc: 0.3160\n",
      "Epoch 425/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5280 - acc: 0.4159 - val_loss: 1.7410 - val_acc: 0.3123\n",
      "Epoch 426/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5294 - acc: 0.4227 - val_loss: 1.7724 - val_acc: 0.2937\n",
      "Epoch 427/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5286 - acc: 0.4270 - val_loss: 1.7543 - val_acc: 0.2974\n",
      "Epoch 428/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5344 - acc: 0.4253 - val_loss: 1.7471 - val_acc: 0.2974\n",
      "Epoch 429/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5283 - acc: 0.4278 - val_loss: 1.7305 - val_acc: 0.3234\n",
      "Epoch 430/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5256 - acc: 0.4313 - val_loss: 1.7311 - val_acc: 0.3160\n",
      "Epoch 431/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5317 - acc: 0.4278 - val_loss: 1.7307 - val_acc: 0.3160\n",
      "Epoch 432/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5254 - acc: 0.4125 - val_loss: 1.8442 - val_acc: 0.2788\n",
      "Epoch 433/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5244 - acc: 0.4236 - val_loss: 1.8129 - val_acc: 0.2900\n",
      "Epoch 434/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5187 - acc: 0.4355 - val_loss: 1.7306 - val_acc: 0.3271\n",
      "Epoch 435/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5265 - acc: 0.4167 - val_loss: 1.7299 - val_acc: 0.3160\n",
      "Epoch 436/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5289 - acc: 0.4295 - val_loss: 1.8037 - val_acc: 0.2825\n",
      "Epoch 437/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5250 - acc: 0.4176 - val_loss: 1.7718 - val_acc: 0.2974\n",
      "Epoch 438/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5282 - acc: 0.4372 - val_loss: 1.7252 - val_acc: 0.3234\n",
      "Epoch 439/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5200 - acc: 0.4330 - val_loss: 2.0218 - val_acc: 0.2639\n",
      "Epoch 440/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5251 - acc: 0.4347 - val_loss: 1.7355 - val_acc: 0.3048\n",
      "Epoch 441/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5250 - acc: 0.4295 - val_loss: 1.7779 - val_acc: 0.3011\n",
      "Epoch 442/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5239 - acc: 0.4176 - val_loss: 1.7684 - val_acc: 0.3123\n",
      "Epoch 443/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5186 - acc: 0.4338 - val_loss: 1.7820 - val_acc: 0.3048\n",
      "Epoch 444/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5237 - acc: 0.4270 - val_loss: 1.8253 - val_acc: 0.2862\n",
      "Epoch 445/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5153 - acc: 0.4321 - val_loss: 1.7267 - val_acc: 0.3197\n",
      "Epoch 446/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5113 - acc: 0.4441 - val_loss: 1.7311 - val_acc: 0.3160\n",
      "Epoch 447/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5180 - acc: 0.4321 - val_loss: 1.7300 - val_acc: 0.3271\n",
      "Epoch 448/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.5157 - acc: 0.4321 - val_loss: 1.7310 - val_acc: 0.3160\n",
      "Epoch 449/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5211 - acc: 0.4466 - val_loss: 1.8735 - val_acc: 0.2825\n",
      "Epoch 450/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5130 - acc: 0.4321 - val_loss: 1.7596 - val_acc: 0.2862\n",
      "Epoch 451/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.5142 - acc: 0.4398 - val_loss: 1.7448 - val_acc: 0.3123\n",
      "Epoch 452/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5108 - acc: 0.4389 - val_loss: 1.7412 - val_acc: 0.3309\n",
      "Epoch 453/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5107 - acc: 0.4372 - val_loss: 1.7547 - val_acc: 0.3011\n",
      "Epoch 454/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5140 - acc: 0.4321 - val_loss: 1.8101 - val_acc: 0.2825\n",
      "Epoch 455/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5115 - acc: 0.4313 - val_loss: 1.8535 - val_acc: 0.2862\n",
      "Epoch 456/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5072 - acc: 0.4313 - val_loss: 1.9379 - val_acc: 0.2677\n",
      "Epoch 457/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5133 - acc: 0.4338 - val_loss: 1.7519 - val_acc: 0.2862\n",
      "Epoch 458/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5056 - acc: 0.4398 - val_loss: 1.8447 - val_acc: 0.2788\n",
      "Epoch 459/700\n",
      "1171/1171 [==============================] - 15s 13ms/step - loss: 1.5051 - acc: 0.4458 - val_loss: 1.7279 - val_acc: 0.3197\n",
      "Epoch 460/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.5117 - acc: 0.4304 - val_loss: 1.8464 - val_acc: 0.2900\n",
      "Epoch 461/700\n",
      "1171/1171 [==============================] - 12s 11ms/step - loss: 1.5056 - acc: 0.4381 - val_loss: 1.7560 - val_acc: 0.2974\n",
      "Epoch 462/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5084 - acc: 0.4253 - val_loss: 1.7206 - val_acc: 0.3234\n",
      "Epoch 463/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5040 - acc: 0.4295 - val_loss: 1.7197 - val_acc: 0.3234\n",
      "Epoch 464/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5031 - acc: 0.4458 - val_loss: 1.8230 - val_acc: 0.2788\n",
      "Epoch 465/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5057 - acc: 0.4236 - val_loss: 1.7497 - val_acc: 0.3086\n",
      "Epoch 466/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5076 - acc: 0.4398 - val_loss: 1.7278 - val_acc: 0.3309\n",
      "Epoch 467/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4981 - acc: 0.4492 - val_loss: 1.7395 - val_acc: 0.3123\n",
      "Epoch 468/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4987 - acc: 0.4295 - val_loss: 1.8377 - val_acc: 0.2825\n",
      "Epoch 469/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4976 - acc: 0.4389 - val_loss: 1.7802 - val_acc: 0.2862\n",
      "Epoch 470/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5052 - acc: 0.4458 - val_loss: 1.7516 - val_acc: 0.2937\n",
      "Epoch 471/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.5047 - acc: 0.4492 - val_loss: 1.7488 - val_acc: 0.3086\n",
      "Epoch 472/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4945 - acc: 0.4321 - val_loss: 1.9149 - val_acc: 0.2751\n",
      "Epoch 473/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4983 - acc: 0.4304 - val_loss: 1.7503 - val_acc: 0.3086\n",
      "Epoch 474/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4981 - acc: 0.4518 - val_loss: 1.7222 - val_acc: 0.3234\n",
      "Epoch 475/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.5017 - acc: 0.4364 - val_loss: 1.7495 - val_acc: 0.3086\n",
      "Epoch 476/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4976 - acc: 0.4355 - val_loss: 1.8706 - val_acc: 0.2825\n",
      "Epoch 477/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4899 - acc: 0.4449 - val_loss: 1.9798 - val_acc: 0.2714\n",
      "Epoch 478/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.4885 - acc: 0.4406 - val_loss: 1.7224 - val_acc: 0.3234\n",
      "Epoch 479/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4922 - acc: 0.4415 - val_loss: 1.7783 - val_acc: 0.3011\n",
      "Epoch 480/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4994 - acc: 0.4398 - val_loss: 1.7132 - val_acc: 0.3346\n",
      "Epoch 481/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.5008 - acc: 0.4347 - val_loss: 1.7463 - val_acc: 0.3086\n",
      "Epoch 482/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4924 - acc: 0.4492 - val_loss: 1.7783 - val_acc: 0.2937\n",
      "Epoch 483/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4940 - acc: 0.4441 - val_loss: 1.7366 - val_acc: 0.3197\n",
      "Epoch 484/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4960 - acc: 0.4347 - val_loss: 1.7229 - val_acc: 0.3160\n",
      "Epoch 485/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4950 - acc: 0.4389 - val_loss: 1.7349 - val_acc: 0.3123\n",
      "Epoch 486/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4941 - acc: 0.4518 - val_loss: 1.8221 - val_acc: 0.2788\n",
      "Epoch 487/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4954 - acc: 0.4424 - val_loss: 1.7362 - val_acc: 0.3123\n",
      "Epoch 488/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4919 - acc: 0.4398 - val_loss: 1.8173 - val_acc: 0.2825\n",
      "Epoch 489/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4860 - acc: 0.4441 - val_loss: 1.7237 - val_acc: 0.3197\n",
      "Epoch 490/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4835 - acc: 0.4441 - val_loss: 1.7289 - val_acc: 0.3123\n",
      "Epoch 491/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4853 - acc: 0.4603 - val_loss: 1.7707 - val_acc: 0.3160\n",
      "Epoch 492/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4855 - acc: 0.4543 - val_loss: 1.8132 - val_acc: 0.2937\n",
      "Epoch 493/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4806 - acc: 0.4415 - val_loss: 1.7429 - val_acc: 0.2974\n",
      "Epoch 494/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4864 - acc: 0.4535 - val_loss: 1.7238 - val_acc: 0.3086\n",
      "Epoch 495/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4880 - acc: 0.4415 - val_loss: 1.7703 - val_acc: 0.2900\n",
      "Epoch 496/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4813 - acc: 0.4552 - val_loss: 1.7305 - val_acc: 0.3123\n",
      "Epoch 497/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4707 - acc: 0.4543 - val_loss: 1.7599 - val_acc: 0.3048\n",
      "Epoch 498/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4828 - acc: 0.4509 - val_loss: 1.8515 - val_acc: 0.2900\n",
      "Epoch 499/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4822 - acc: 0.4500 - val_loss: 1.7809 - val_acc: 0.3048\n",
      "Epoch 500/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4832 - acc: 0.4432 - val_loss: 1.8618 - val_acc: 0.2900\n",
      "Epoch 501/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4816 - acc: 0.4389 - val_loss: 1.8324 - val_acc: 0.2900\n",
      "Epoch 502/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4759 - acc: 0.4441 - val_loss: 1.7188 - val_acc: 0.3234\n",
      "Epoch 503/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4809 - acc: 0.4372 - val_loss: 1.8285 - val_acc: 0.2900\n",
      "Epoch 504/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4704 - acc: 0.4321 - val_loss: 1.8033 - val_acc: 0.2974\n",
      "Epoch 505/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4776 - acc: 0.4398 - val_loss: 1.7237 - val_acc: 0.3271\n",
      "Epoch 506/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4719 - acc: 0.4415 - val_loss: 1.8747 - val_acc: 0.2825\n",
      "Epoch 507/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4779 - acc: 0.4560 - val_loss: 1.7692 - val_acc: 0.3086\n",
      "Epoch 508/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4722 - acc: 0.4475 - val_loss: 1.7169 - val_acc: 0.3197\n",
      "Epoch 509/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4727 - acc: 0.4526 - val_loss: 1.8074 - val_acc: 0.2974\n",
      "Epoch 510/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4781 - acc: 0.4398 - val_loss: 1.7265 - val_acc: 0.3160\n",
      "Epoch 511/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4745 - acc: 0.4569 - val_loss: 1.7587 - val_acc: 0.3048\n",
      "Epoch 512/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4796 - acc: 0.4441 - val_loss: 1.8084 - val_acc: 0.2900\n",
      "Epoch 513/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4761 - acc: 0.4637 - val_loss: 1.8248 - val_acc: 0.2974\n",
      "Epoch 514/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4638 - acc: 0.4543 - val_loss: 1.7312 - val_acc: 0.3086\n",
      "Epoch 515/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4672 - acc: 0.4535 - val_loss: 1.7433 - val_acc: 0.3123\n",
      "Epoch 516/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4722 - acc: 0.4569 - val_loss: 1.7373 - val_acc: 0.3123\n",
      "Epoch 517/700\n",
      "1171/1171 [==============================] - 12s 11ms/step - loss: 1.4694 - acc: 0.4535 - val_loss: 1.7519 - val_acc: 0.3048\n",
      "Epoch 518/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4618 - acc: 0.4637 - val_loss: 1.7966 - val_acc: 0.2937\n",
      "Epoch 519/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4717 - acc: 0.4535 - val_loss: 1.7364 - val_acc: 0.3123\n",
      "Epoch 520/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4646 - acc: 0.4526 - val_loss: 1.7960 - val_acc: 0.2937\n",
      "Epoch 521/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4669 - acc: 0.4518 - val_loss: 1.7410 - val_acc: 0.3011\n",
      "Epoch 522/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4627 - acc: 0.4552 - val_loss: 1.7633 - val_acc: 0.2974\n",
      "Epoch 523/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4584 - acc: 0.4569 - val_loss: 1.7205 - val_acc: 0.3123\n",
      "Epoch 524/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4673 - acc: 0.4611 - val_loss: 1.7998 - val_acc: 0.2974\n",
      "Epoch 525/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.4563 - acc: 0.4663 - val_loss: 1.8699 - val_acc: 0.2862\n",
      "Epoch 526/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4626 - acc: 0.4518 - val_loss: 1.7125 - val_acc: 0.3271\n",
      "Epoch 527/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4653 - acc: 0.4432 - val_loss: 1.7901 - val_acc: 0.3048\n",
      "Epoch 528/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4518 - acc: 0.4509 - val_loss: 1.8577 - val_acc: 0.2974\n",
      "Epoch 529/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4607 - acc: 0.4458 - val_loss: 1.9649 - val_acc: 0.2677\n",
      "Epoch 530/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4654 - acc: 0.4740 - val_loss: 1.7341 - val_acc: 0.3011\n",
      "Epoch 531/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4507 - acc: 0.4603 - val_loss: 1.7240 - val_acc: 0.3086\n",
      "Epoch 532/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4596 - acc: 0.4449 - val_loss: 1.7145 - val_acc: 0.3346\n",
      "Epoch 533/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4556 - acc: 0.4569 - val_loss: 1.7026 - val_acc: 0.3383\n",
      "Epoch 534/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4581 - acc: 0.4603 - val_loss: 1.7191 - val_acc: 0.3309\n",
      "Epoch 535/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4620 - acc: 0.4680 - val_loss: 1.7275 - val_acc: 0.3234\n",
      "Epoch 536/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4565 - acc: 0.4492 - val_loss: 1.7169 - val_acc: 0.3197\n",
      "Epoch 537/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4535 - acc: 0.4603 - val_loss: 1.7181 - val_acc: 0.3420\n",
      "Epoch 538/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4543 - acc: 0.4722 - val_loss: 1.7170 - val_acc: 0.3160\n",
      "Epoch 539/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4537 - acc: 0.4603 - val_loss: 1.7251 - val_acc: 0.3346\n",
      "Epoch 540/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4459 - acc: 0.4765 - val_loss: 1.7514 - val_acc: 0.3197\n",
      "Epoch 541/700\n",
      "1171/1171 [==============================] - 12s 11ms/step - loss: 1.4572 - acc: 0.4646 - val_loss: 1.9788 - val_acc: 0.2677\n",
      "Epoch 542/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4485 - acc: 0.4415 - val_loss: 1.8192 - val_acc: 0.2937\n",
      "Epoch 543/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4532 - acc: 0.4526 - val_loss: 1.8144 - val_acc: 0.3011\n",
      "Epoch 544/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4570 - acc: 0.4535 - val_loss: 1.7350 - val_acc: 0.3011\n",
      "Epoch 545/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4529 - acc: 0.4714 - val_loss: 1.7451 - val_acc: 0.3123\n",
      "Epoch 546/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4417 - acc: 0.4697 - val_loss: 1.7513 - val_acc: 0.3048\n",
      "Epoch 547/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4414 - acc: 0.4637 - val_loss: 1.7202 - val_acc: 0.3346\n",
      "Epoch 548/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4483 - acc: 0.4458 - val_loss: 1.7389 - val_acc: 0.3197\n",
      "Epoch 549/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4435 - acc: 0.4543 - val_loss: 1.7517 - val_acc: 0.2937\n",
      "Epoch 550/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4464 - acc: 0.4518 - val_loss: 1.7256 - val_acc: 0.3123\n",
      "Epoch 551/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4363 - acc: 0.4560 - val_loss: 1.7393 - val_acc: 0.3160\n",
      "Epoch 552/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4539 - acc: 0.4466 - val_loss: 1.7535 - val_acc: 0.3048\n",
      "Epoch 553/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4429 - acc: 0.4637 - val_loss: 1.7476 - val_acc: 0.3048\n",
      "Epoch 554/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4467 - acc: 0.4654 - val_loss: 1.7506 - val_acc: 0.3048\n",
      "Epoch 555/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4443 - acc: 0.4594 - val_loss: 1.7669 - val_acc: 0.2974\n",
      "Epoch 556/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4435 - acc: 0.4671 - val_loss: 1.8369 - val_acc: 0.2937\n",
      "Epoch 557/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4356 - acc: 0.4688 - val_loss: 1.7077 - val_acc: 0.3383\n",
      "Epoch 558/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4360 - acc: 0.4748 - val_loss: 1.7143 - val_acc: 0.3346\n",
      "Epoch 559/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4418 - acc: 0.4637 - val_loss: 1.7354 - val_acc: 0.3197\n",
      "Epoch 560/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4406 - acc: 0.4552 - val_loss: 1.7568 - val_acc: 0.3086\n",
      "Epoch 561/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4347 - acc: 0.4569 - val_loss: 1.7175 - val_acc: 0.3160\n",
      "Epoch 562/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4453 - acc: 0.4500 - val_loss: 1.8392 - val_acc: 0.2900\n",
      "Epoch 563/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4338 - acc: 0.4629 - val_loss: 1.8175 - val_acc: 0.2900\n",
      "Epoch 564/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4339 - acc: 0.4552 - val_loss: 1.7179 - val_acc: 0.3457\n",
      "Epoch 565/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4326 - acc: 0.4663 - val_loss: 1.8544 - val_acc: 0.2937\n",
      "Epoch 566/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4381 - acc: 0.4680 - val_loss: 1.7157 - val_acc: 0.3271\n",
      "Epoch 567/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4384 - acc: 0.4492 - val_loss: 1.7285 - val_acc: 0.3160\n",
      "Epoch 568/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4324 - acc: 0.4611 - val_loss: 1.7993 - val_acc: 0.2937\n",
      "Epoch 569/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4259 - acc: 0.4740 - val_loss: 1.7807 - val_acc: 0.3048\n",
      "Epoch 570/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4405 - acc: 0.4705 - val_loss: 1.7218 - val_acc: 0.3309\n",
      "Epoch 571/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4287 - acc: 0.4560 - val_loss: 1.8956 - val_acc: 0.2862\n",
      "Epoch 572/700\n",
      "1171/1171 [==============================] - 12s 11ms/step - loss: 1.4300 - acc: 0.4560 - val_loss: 1.7348 - val_acc: 0.3234\n",
      "Epoch 573/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4335 - acc: 0.4671 - val_loss: 1.7173 - val_acc: 0.3383\n",
      "Epoch 574/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4223 - acc: 0.4757 - val_loss: 1.7111 - val_acc: 0.3309\n",
      "Epoch 575/700\n",
      "1171/1171 [==============================] - 12s 11ms/step - loss: 1.4310 - acc: 0.4629 - val_loss: 1.7087 - val_acc: 0.3271\n",
      "Epoch 576/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4276 - acc: 0.4705 - val_loss: 1.7408 - val_acc: 0.3011\n",
      "Epoch 577/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4227 - acc: 0.4671 - val_loss: 1.7536 - val_acc: 0.3086\n",
      "Epoch 578/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4303 - acc: 0.4722 - val_loss: 1.7065 - val_acc: 0.3309\n",
      "Epoch 579/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4224 - acc: 0.4722 - val_loss: 1.7230 - val_acc: 0.3271\n",
      "Epoch 580/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4233 - acc: 0.4620 - val_loss: 1.8314 - val_acc: 0.3011\n",
      "Epoch 581/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4343 - acc: 0.4586 - val_loss: 1.7863 - val_acc: 0.3048\n",
      "Epoch 582/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4307 - acc: 0.4611 - val_loss: 1.7707 - val_acc: 0.3048\n",
      "Epoch 583/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4267 - acc: 0.4740 - val_loss: 1.7821 - val_acc: 0.2900\n",
      "Epoch 584/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4268 - acc: 0.4637 - val_loss: 1.7495 - val_acc: 0.3048\n",
      "Epoch 585/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4237 - acc: 0.4833 - val_loss: 1.7527 - val_acc: 0.3048\n",
      "Epoch 586/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4108 - acc: 0.4722 - val_loss: 1.7886 - val_acc: 0.2974\n",
      "Epoch 587/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4243 - acc: 0.4620 - val_loss: 1.7377 - val_acc: 0.3011\n",
      "Epoch 588/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4240 - acc: 0.4586 - val_loss: 1.9019 - val_acc: 0.2937\n",
      "Epoch 589/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4227 - acc: 0.4714 - val_loss: 1.7147 - val_acc: 0.3123\n",
      "Epoch 590/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4276 - acc: 0.4611 - val_loss: 1.7267 - val_acc: 0.3197\n",
      "Epoch 591/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4209 - acc: 0.4799 - val_loss: 1.8006 - val_acc: 0.2974\n",
      "Epoch 592/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4260 - acc: 0.4654 - val_loss: 1.7543 - val_acc: 0.3048\n",
      "Epoch 593/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4155 - acc: 0.4748 - val_loss: 1.7102 - val_acc: 0.3234\n",
      "Epoch 594/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4108 - acc: 0.4646 - val_loss: 1.7371 - val_acc: 0.3086\n",
      "Epoch 595/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4210 - acc: 0.4629 - val_loss: 1.7332 - val_acc: 0.3197\n",
      "Epoch 596/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4113 - acc: 0.4808 - val_loss: 1.7354 - val_acc: 0.3086\n",
      "Epoch 597/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4104 - acc: 0.4782 - val_loss: 1.8035 - val_acc: 0.3086\n",
      "Epoch 598/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4142 - acc: 0.4671 - val_loss: 1.7825 - val_acc: 0.2937\n",
      "Epoch 599/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4101 - acc: 0.4740 - val_loss: 1.7908 - val_acc: 0.2937\n",
      "Epoch 600/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4138 - acc: 0.4757 - val_loss: 1.7052 - val_acc: 0.3346\n",
      "Epoch 601/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4194 - acc: 0.4714 - val_loss: 1.8140 - val_acc: 0.3123\n",
      "Epoch 602/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4132 - acc: 0.4782 - val_loss: 1.7615 - val_acc: 0.3048\n",
      "Epoch 603/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4139 - acc: 0.4859 - val_loss: 1.7110 - val_acc: 0.3383\n",
      "Epoch 604/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4173 - acc: 0.4697 - val_loss: 1.7108 - val_acc: 0.3271\n",
      "Epoch 605/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4058 - acc: 0.4757 - val_loss: 1.7439 - val_acc: 0.3160\n",
      "Epoch 606/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4210 - acc: 0.4774 - val_loss: 1.7363 - val_acc: 0.3123\n",
      "Epoch 607/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4164 - acc: 0.4808 - val_loss: 1.7509 - val_acc: 0.3160\n",
      "Epoch 608/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4122 - acc: 0.4757 - val_loss: 1.7157 - val_acc: 0.3383\n",
      "Epoch 609/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4071 - acc: 0.4629 - val_loss: 1.7610 - val_acc: 0.3048\n",
      "Epoch 610/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4023 - acc: 0.4919 - val_loss: 1.9102 - val_acc: 0.2937\n",
      "Epoch 611/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4071 - acc: 0.4748 - val_loss: 1.8296 - val_acc: 0.2974\n",
      "Epoch 612/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4099 - acc: 0.4722 - val_loss: 1.7504 - val_acc: 0.3048\n",
      "Epoch 613/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4079 - acc: 0.4825 - val_loss: 1.7564 - val_acc: 0.3011\n",
      "Epoch 614/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4106 - acc: 0.4688 - val_loss: 1.7269 - val_acc: 0.3048\n",
      "Epoch 615/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4032 - acc: 0.4757 - val_loss: 1.7520 - val_acc: 0.2974\n",
      "Epoch 616/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4051 - acc: 0.4731 - val_loss: 1.7468 - val_acc: 0.3234\n",
      "Epoch 617/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3975 - acc: 0.4654 - val_loss: 1.7041 - val_acc: 0.3197\n",
      "Epoch 618/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4009 - acc: 0.4799 - val_loss: 1.7269 - val_acc: 0.3309\n",
      "Epoch 619/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3974 - acc: 0.4808 - val_loss: 1.7150 - val_acc: 0.3309\n",
      "Epoch 620/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3988 - acc: 0.4808 - val_loss: 1.7092 - val_acc: 0.3420\n",
      "Epoch 621/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4023 - acc: 0.4654 - val_loss: 1.7370 - val_acc: 0.3234\n",
      "Epoch 622/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3992 - acc: 0.4740 - val_loss: 1.7102 - val_acc: 0.3383\n",
      "Epoch 623/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.4007 - acc: 0.4654 - val_loss: 1.7335 - val_acc: 0.3048\n",
      "Epoch 624/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3954 - acc: 0.4697 - val_loss: 1.7078 - val_acc: 0.3309\n",
      "Epoch 625/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3940 - acc: 0.4936 - val_loss: 1.8063 - val_acc: 0.3011\n",
      "Epoch 626/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3931 - acc: 0.4859 - val_loss: 1.7062 - val_acc: 0.3420\n",
      "Epoch 627/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4014 - acc: 0.4799 - val_loss: 1.7320 - val_acc: 0.3123\n",
      "Epoch 628/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3881 - acc: 0.4902 - val_loss: 1.8097 - val_acc: 0.3011\n",
      "Epoch 629/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.4011 - acc: 0.4680 - val_loss: 1.9046 - val_acc: 0.2937\n",
      "Epoch 630/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3903 - acc: 0.4859 - val_loss: 1.7331 - val_acc: 0.3048\n",
      "Epoch 631/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3950 - acc: 0.4722 - val_loss: 1.7124 - val_acc: 0.3309\n",
      "Epoch 632/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.3948 - acc: 0.4791 - val_loss: 1.7117 - val_acc: 0.3309\n",
      "Epoch 633/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3904 - acc: 0.4791 - val_loss: 1.7246 - val_acc: 0.3123\n",
      "Epoch 634/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.3883 - acc: 0.4774 - val_loss: 1.7200 - val_acc: 0.3234\n",
      "Epoch 635/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3951 - acc: 0.4799 - val_loss: 1.7179 - val_acc: 0.3383\n",
      "Epoch 636/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3777 - acc: 0.4962 - val_loss: 1.7645 - val_acc: 0.3123\n",
      "Epoch 637/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3805 - acc: 0.4979 - val_loss: 1.8295 - val_acc: 0.3123\n",
      "Epoch 638/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3924 - acc: 0.4654 - val_loss: 1.7118 - val_acc: 0.3383\n",
      "Epoch 639/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.4007 - acc: 0.4714 - val_loss: 1.7187 - val_acc: 0.3309\n",
      "Epoch 640/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.3858 - acc: 0.4808 - val_loss: 1.7187 - val_acc: 0.3383\n",
      "Epoch 641/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.3870 - acc: 0.4816 - val_loss: 1.7113 - val_acc: 0.3197\n",
      "Epoch 642/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3912 - acc: 0.4859 - val_loss: 1.7282 - val_acc: 0.3048\n",
      "Epoch 643/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.3836 - acc: 0.4902 - val_loss: 1.6967 - val_acc: 0.3346\n",
      "Epoch 644/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3794 - acc: 0.5004 - val_loss: 1.7191 - val_acc: 0.3234\n",
      "Epoch 645/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3877 - acc: 0.4902 - val_loss: 1.7146 - val_acc: 0.3420\n",
      "Epoch 646/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3852 - acc: 0.4791 - val_loss: 1.8193 - val_acc: 0.3160\n",
      "Epoch 647/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3832 - acc: 0.4944 - val_loss: 1.7310 - val_acc: 0.3160\n",
      "Epoch 648/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3936 - acc: 0.4902 - val_loss: 1.7212 - val_acc: 0.3271\n",
      "Epoch 649/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3767 - acc: 0.4979 - val_loss: 1.7096 - val_acc: 0.3309\n",
      "Epoch 650/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3881 - acc: 0.4782 - val_loss: 1.7145 - val_acc: 0.3420\n",
      "Epoch 651/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3703 - acc: 0.4936 - val_loss: 1.7110 - val_acc: 0.3234\n",
      "Epoch 652/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3752 - acc: 0.4851 - val_loss: 1.7547 - val_acc: 0.2974\n",
      "Epoch 653/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3758 - acc: 0.4936 - val_loss: 1.7106 - val_acc: 0.3346\n",
      "Epoch 654/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3723 - acc: 0.4893 - val_loss: 1.7857 - val_acc: 0.3123\n",
      "Epoch 655/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3792 - acc: 0.5030 - val_loss: 1.8901 - val_acc: 0.3086\n",
      "Epoch 656/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3702 - acc: 0.4910 - val_loss: 1.7505 - val_acc: 0.3086\n",
      "Epoch 657/700\n",
      "1171/1171 [==============================] - 14s 12ms/step - loss: 1.3606 - acc: 0.4885 - val_loss: 1.7131 - val_acc: 0.3346\n",
      "Epoch 658/700\n",
      "1171/1171 [==============================] - 15s 13ms/step - loss: 1.3807 - acc: 0.4816 - val_loss: 1.7355 - val_acc: 0.3271\n",
      "Epoch 659/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.3743 - acc: 0.4885 - val_loss: 1.7709 - val_acc: 0.3011\n",
      "Epoch 660/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.3728 - acc: 0.4808 - val_loss: 1.7051 - val_acc: 0.3457\n",
      "Epoch 661/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.3661 - acc: 0.4714 - val_loss: 1.7068 - val_acc: 0.3532\n",
      "Epoch 662/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3744 - acc: 0.4893 - val_loss: 1.8335 - val_acc: 0.2900\n",
      "Epoch 663/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3665 - acc: 0.5021 - val_loss: 1.7550 - val_acc: 0.3048\n",
      "Epoch 664/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3721 - acc: 0.4902 - val_loss: 1.7058 - val_acc: 0.3420\n",
      "Epoch 665/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3702 - acc: 0.4970 - val_loss: 1.7600 - val_acc: 0.2974\n",
      "Epoch 666/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.3669 - acc: 0.4910 - val_loss: 1.7320 - val_acc: 0.3160\n",
      "Epoch 667/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3656 - acc: 0.4919 - val_loss: 1.7157 - val_acc: 0.3197\n",
      "Epoch 668/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.3683 - acc: 0.4842 - val_loss: 1.7979 - val_acc: 0.2862\n",
      "Epoch 669/700\n",
      "1171/1171 [==============================] - 13s 11ms/step - loss: 1.3676 - acc: 0.4902 - val_loss: 1.7586 - val_acc: 0.3123\n",
      "Epoch 670/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3724 - acc: 0.4910 - val_loss: 1.7660 - val_acc: 0.2974\n",
      "Epoch 671/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3624 - acc: 0.5004 - val_loss: 1.8393 - val_acc: 0.2974\n",
      "Epoch 672/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3619 - acc: 0.4996 - val_loss: 1.7191 - val_acc: 0.3309\n",
      "Epoch 673/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3640 - acc: 0.4842 - val_loss: 1.7069 - val_acc: 0.3346\n",
      "Epoch 674/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.3658 - acc: 0.4842 - val_loss: 1.7323 - val_acc: 0.3123\n",
      "Epoch 675/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3676 - acc: 0.4919 - val_loss: 1.7004 - val_acc: 0.3420\n",
      "Epoch 676/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3688 - acc: 0.4902 - val_loss: 1.7182 - val_acc: 0.3346\n",
      "Epoch 677/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3681 - acc: 0.5073 - val_loss: 1.7310 - val_acc: 0.3197\n",
      "Epoch 678/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3607 - acc: 0.4962 - val_loss: 1.9051 - val_acc: 0.2825\n",
      "Epoch 679/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3614 - acc: 0.4962 - val_loss: 1.9058 - val_acc: 0.2825\n",
      "Epoch 680/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3556 - acc: 0.4970 - val_loss: 1.7543 - val_acc: 0.3048\n",
      "Epoch 681/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3698 - acc: 0.5021 - val_loss: 1.7430 - val_acc: 0.3160\n",
      "Epoch 682/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3598 - acc: 0.4944 - val_loss: 1.7246 - val_acc: 0.3197\n",
      "Epoch 683/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3624 - acc: 0.5004 - val_loss: 1.7687 - val_acc: 0.2974\n",
      "Epoch 684/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3522 - acc: 0.5021 - val_loss: 1.7766 - val_acc: 0.2937\n",
      "Epoch 685/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3560 - acc: 0.4816 - val_loss: 1.7696 - val_acc: 0.2900\n",
      "Epoch 686/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3569 - acc: 0.4927 - val_loss: 1.7257 - val_acc: 0.3086\n",
      "Epoch 687/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3501 - acc: 0.4996 - val_loss: 1.7279 - val_acc: 0.3160\n",
      "Epoch 688/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3627 - acc: 0.4876 - val_loss: 1.7607 - val_acc: 0.3011\n",
      "Epoch 689/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3543 - acc: 0.4927 - val_loss: 1.7395 - val_acc: 0.3234\n",
      "Epoch 690/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3547 - acc: 0.5021 - val_loss: 1.8060 - val_acc: 0.3086\n",
      "Epoch 691/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3403 - acc: 0.4868 - val_loss: 1.7094 - val_acc: 0.3271\n",
      "Epoch 692/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.3566 - acc: 0.4927 - val_loss: 1.7549 - val_acc: 0.2974\n",
      "Epoch 693/700\n",
      "1171/1171 [==============================] - 12s 10ms/step - loss: 1.3493 - acc: 0.4859 - val_loss: 1.7259 - val_acc: 0.3160\n",
      "Epoch 694/700\n",
      "1171/1171 [==============================] - 12s 11ms/step - loss: 1.3465 - acc: 0.5013 - val_loss: 1.7347 - val_acc: 0.3271\n",
      "Epoch 695/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3547 - acc: 0.4902 - val_loss: 1.7405 - val_acc: 0.3086\n",
      "Epoch 696/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3467 - acc: 0.5098 - val_loss: 1.7060 - val_acc: 0.3383\n",
      "Epoch 697/700\n",
      "1171/1171 [==============================] - 11s 9ms/step - loss: 1.3483 - acc: 0.5141 - val_loss: 1.9159 - val_acc: 0.2937\n",
      "Epoch 698/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3554 - acc: 0.4902 - val_loss: 1.8326 - val_acc: 0.3123\n",
      "Epoch 699/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3451 - acc: 0.5098 - val_loss: 1.8037 - val_acc: 0.3011\n",
      "Epoch 700/700\n",
      "1171/1171 [==============================] - 11s 10ms/step - loss: 1.3494 - acc: 0.4927 - val_loss: 1.7555 - val_acc: 0.3048\n"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
